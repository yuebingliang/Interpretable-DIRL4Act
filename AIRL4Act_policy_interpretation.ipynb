{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "YiV7sKPhb7MX"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from torch import nn\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0D-dpwMkWC6C",
    "outputId": "f0a745a6-ad3c-4be3-fb0c-acb79660c15d"
   },
   "outputs": [],
   "source": [
    "# device = \"cpu\"\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load AIRL model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "siiz9OngcKHN"
   },
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "L-w2QfmxcMge"
   },
   "outputs": [],
   "source": [
    "# Step = namedtuple('Step', 'cur_state cur_time cur_dur next_state next_time reward done ind_feat ind_emp')\n",
    "\n",
    "class ActEnvironment(object):\n",
    "  def __init__(self, time_size, pad_activity_id=5, pad_time_id=96, pad_dur_id=96, pad_traj_len_id=96, pad_dur_leave_home_id=96, pad_dur_travel_id=96):\n",
    "    self.time_size = time_size\n",
    "    self.pad_activity_id = pad_activity_id\n",
    "    self.home_activity_id = 0\n",
    "    self.travel_activity_id = pad_activity_id-1\n",
    "    self.pad_time_id = pad_time_id\n",
    "    self.pad_dur_id = pad_dur_id\n",
    "    self.pad_traj_len_id = pad_traj_len_id\n",
    "    self.pad_dur_leave_home_id = pad_dur_leave_home_id\n",
    "    self.pad_dur_travel_id = pad_dur_travel_id\n",
    "    self.rewards = np.zeros((self.pad_activity_id + 1, self.pad_time_id + 1, self.pad_dur_id + 1))\n",
    "\n",
    "  def get_reward(self, activity, time, dur):\n",
    "    return self.rewards[activity, time, dur]\n",
    "\n",
    "  def reset(self, ind_feat=None, ind_employ=None):\n",
    "    if ind_feat is not None:\n",
    "      cur_ind_feat = ind_feat\n",
    "      cur_ind_employ = ind_employ\n",
    "    else:\n",
    "      sample_idx = np.random.choice(len(self.ind_feat))\n",
    "      cur_ind_feat = self.ind_feat[sample_idx]\n",
    "      cur_ind_employ = self.ind_emp[sample_idx]\n",
    "\n",
    "    return self.pad_activity_id, self.pad_time_id, self.pad_dur_id, self.pad_traj_len_id, self.pad_dur_leave_home_id, self.pad_dur_travel_id, cur_ind_feat, cur_ind_employ\n",
    "\n",
    "  # def get_current_state(self):\n",
    "  #   return self._cur_state, self._cur_time, self._cur_ind_feat, self._cur_ind_employ\n",
    "\n",
    "  def step(self, activity, time, dur, traj_len, dur_leave_home, dur_travel, next_activity):\n",
    "    next_time = time + 1 if time < self.pad_time_id else 0\n",
    "    action = 0 if next_activity == activity else 1\n",
    "    next_dur = 0 if action == 1 else dur + 1\n",
    "    next_traj_len = traj_len if action == 0 else (traj_len + 1 if traj_len != self.pad_dur_travel_id else 0)\n",
    "    if next_activity == self.home_activity_id:\n",
    "      next_dur_leave_home = self.pad_dur_leave_home_id\n",
    "    else:\n",
    "      if activity in [self.home_activity_id, self.pad_activity_id]:\n",
    "        next_dur_leave_home = 0\n",
    "      else:\n",
    "        next_dur_leave_home = dur_leave_home + 1\n",
    "    if next_activity == self.travel_activity_id:\n",
    "      if dur_travel == self.pad_dur_travel_id:\n",
    "        next_dur_travel = 0\n",
    "      else:\n",
    "        next_dur_travel = dur_travel + 1\n",
    "    else:\n",
    "      next_dur_travel = dur_travel\n",
    "    reward = self.get_reward(activity, time, dur)\n",
    "    if next_time + 1 == self.time_size:\n",
    "      return next_time, next_dur, next_traj_len, next_dur_leave_home, next_dur_travel, action, reward, True\n",
    "    return next_time, next_dur, next_traj_len, next_dur_leave_home, next_dur_travel, action, reward, False\n",
    "\n",
    "  def import_demonstrations(self, train_p, test_p, traj_col):\n",
    "    train_df = pd.read_csv(train_p)\n",
    "    # train_df = train_df.loc[train_df[filter_col] > 0].copy()\n",
    "    test_df = pd.read_csv(test_p)\n",
    "    # test_df = train_df.loc[test_df[filter_col] > 0].copy()\n",
    "    train_trajs, test_trajs = [], []\n",
    "\n",
    "    train_ind_feat = train_df[[\"age\", \"gender\", \"car\", \"income\", \"workday\"]].values\n",
    "    test_ind_feat = test_df[[\"age\", \"gender\", \"car\", \"income\", \"workday\"]].values\n",
    "    ind_feat = np.concatenate([train_ind_feat, test_ind_feat], 0)\n",
    "    min_ind_feat, max_ind_feat = np.expand_dims(np.min(ind_feat, 0), 0), np.expand_dims(np.max(ind_feat, 0), 0)\n",
    "    train_ind_feat = (train_ind_feat - min_ind_feat) / (max_ind_feat - min_ind_feat)\n",
    "    train_ind_emp = train_df[\"employ\"].values\n",
    "    self.ind_feat = train_ind_feat\n",
    "    self.ind_emp = train_ind_emp\n",
    "    train_demo_str_ls = train_df[traj_col].tolist()\n",
    "\n",
    "    expert_activity, expert_time, expert_dur, expert_traj_len, expert_dur_leave_home, expert_dur_travel = [], [], [], [], [], []\n",
    "    expert_next_activity, expert_next_time, expert_next_dur, expert_next_traj_len, expert_next_dur_leave_home, expert_next_dur_travel = [], [], [], [], [], []\n",
    "    expert_action, expert_ind_feat, expert_ind_emp = [], [], []\n",
    "\n",
    "    for user in range(len(train_demo_str_ls)):\n",
    "      train_demo_ls = [int(demo) for demo in train_demo_str_ls[user].split(\"_\")]\n",
    "      activity, time, dur, traj_len, dur_leave_home, dur_travel = self.pad_activity_id, self.pad_time_id, self.pad_dur_id, \\\n",
    "            self.pad_traj_len_id, self.pad_dur_leave_home_id, self.pad_dur_travel_id\n",
    "      for i0 in range(len(train_demo_ls)):\n",
    "        next_activity = train_demo_ls[i0]\n",
    "        next_time, next_dur, next_traj_len, next_dur_leave_home, next_dur_travel, action, _, _ = self.step(activity, time, dur, traj_len, dur_leave_home, dur_travel, next_activity)\n",
    "        expert_activity.append(activity)\n",
    "        expert_time.append(time)\n",
    "        expert_dur.append(dur)\n",
    "        expert_traj_len.append(traj_len)\n",
    "        expert_dur_leave_home.append(dur_leave_home)\n",
    "        expert_dur_travel.append(dur_travel)\n",
    "        expert_action.append(action)\n",
    "        expert_next_activity.append(next_activity)\n",
    "        expert_next_time.append(next_time)\n",
    "        expert_next_dur.append(next_dur)\n",
    "        expert_next_traj_len.append(next_traj_len)\n",
    "        expert_next_dur_leave_home.append(next_dur_leave_home)\n",
    "        expert_next_dur_travel.append(next_dur_travel)\n",
    "        expert_ind_feat.append(train_ind_feat[user])\n",
    "        expert_ind_emp.append(train_ind_emp[user])\n",
    "        activity, time, dur, traj_len, dur_leave_home, dur_travel = next_activity, next_time, next_dur, next_traj_len, next_dur_leave_home, next_dur_travel\n",
    "        \n",
    "    return torch.LongTensor(expert_activity), torch.LongTensor(expert_time), torch.LongTensor(expert_dur), \\\n",
    "        torch.LongTensor(expert_traj_len), torch.LongTensor(expert_dur_leave_home), torch.LongTensor(expert_dur_travel), \\\n",
    "        torch.LongTensor(expert_action), torch.LongTensor(expert_next_activity), torch.LongTensor(expert_next_time), torch.LongTensor(expert_next_dur), \\\n",
    "        torch.LongTensor(expert_next_traj_len), torch.LongTensor(expert_next_dur_leave_home), torch.LongTensor(expert_next_dur_travel), \\\n",
    "        torch.FloatTensor(np.array(expert_ind_feat)), torch.LongTensor(np.array(expert_ind_emp))\n",
    "\n",
    "  def load_test_traj(self, train_p, test_p, traj_col):\n",
    "    train_df = pd.read_csv(train_p)\n",
    "    # train_df = train_df.loc[train_df[filter_col] > 0].copy()\n",
    "    test_df = pd.read_csv(test_p)\n",
    "    # test_df = train_df.loc[test_df[filter_col] > 0].copy()\n",
    "    train_trajs, test_trajs = [], []\n",
    "\n",
    "    train_ind_feat = train_df[[\"age\", \"gender\", \"car\", \"income\", \"workday\"]].values\n",
    "    test_ind_feat = test_df[[\"age\", \"gender\", \"car\", \"income\", \"workday\"]].values\n",
    "    ind_feat = np.concatenate([train_ind_feat, test_ind_feat], 0)\n",
    "    min_ind_feat, max_ind_feat = np.expand_dims(np.min(ind_feat, 0), 0), np.expand_dims(np.max(ind_feat, 0), 0)\n",
    "\n",
    "    # test_df = test_df.sample(n=min(1000, len(test_df)))\n",
    "    test_ind_feat = test_df[[\"age\", \"gender\", \"car\", \"income\", \"workday\"]].values\n",
    "    test_ind_feat = (test_ind_feat - min_ind_feat) / (max_ind_feat - min_ind_feat)\n",
    "    test_ind_emp = test_df[\"employ\"].values\n",
    "    test_demo_str_ls = test_df[traj_col].tolist()\n",
    "    test_traj = [[int(i) for i in path.split('_')] for path in test_demo_str_ls]\n",
    "    return np.array(test_traj), test_ind_feat, test_ind_emp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hOtkMuaYdwxK"
   },
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "cyNoCVIIdhMR"
   },
   "outputs": [],
   "source": [
    "\"\"\"evaluation\"\"\"\n",
    "import editdistance\n",
    "from sklearn.metrics import f1_score\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from scipy.spatial import distance\n",
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "\n",
    "smoothie = SmoothingFunction().method1\n",
    "\n",
    "def acc(gen_seq, tar_seq):\n",
    "    return np.sum(gen_seq == tar_seq) / (gen_seq.shape[0] * gen_seq.shape[1])\n",
    "\n",
    "def f1(gen_seq, tar_seq):\n",
    "    return f1_score(tar_seq.reshape(-1), gen_seq.reshape(-1), average='macro')\n",
    "\n",
    "def edit_dist(gen_seq, tar_seq):\n",
    "    edit_dist_list = []\n",
    "    for i in range(tar_seq.shape[0]):\n",
    "        tar_sequence = [str(k) for k in tar_seq[i].tolist()]\n",
    "        gen_sequence = [str(k) for k in gen_seq[i].tolist()]\n",
    "        edit_dist = editdistance.eval(tar_sequence, gen_sequence) / len(tar_sequence)\n",
    "        edit_dist_list.append(edit_dist)\n",
    "    return np.mean(edit_dist_list)\n",
    "\n",
    "\n",
    "def bleu_score(gen_seq, tar_seq):\n",
    "    bleu_score_list = []\n",
    "    for i in range(tar_seq.shape[0]):\n",
    "        tar_sequence = [str(k) for k in tar_seq[i].tolist()]\n",
    "        gen_sequence = [str(k) for k in gen_seq[i].tolist()]\n",
    "        bleu_sc = sentence_bleu([tar_sequence], gen_sequence, smoothing_function=smoothie)\n",
    "        bleu_score_list.append(bleu_sc)\n",
    "    return np.mean(bleu_score_list)\n",
    "\n",
    "def dataset_jsd(gen_seq, tar_seq):\n",
    "    test_trajs_str = ['_'.join([str(k) for k in tar_seq[i].tolist()]) for i in range(len(tar_seq))]\n",
    "    test_trajs_set = set(test_trajs_str)\n",
    "    test_trajs_dict = dict(zip(list(test_trajs_set), range(len(test_trajs_set))))\n",
    "    test_trajs_label = [test_trajs_dict[traj] for traj in test_trajs_str]\n",
    "    test_trajs_label.append(0)\n",
    "    test_p = np.histogram(test_trajs_label)[0] / len(test_trajs_label)\n",
    "\n",
    "    pad_idx = len(test_trajs_set)\n",
    "    learner_trajs_str = ['_'.join([str(k) for k in gen_seq[i].tolist()]) for i in range(len(gen_seq))]\n",
    "    learner_trajs_label = [test_trajs_dict.get(traj, pad_idx) for traj in learner_trajs_str]\n",
    "    learner_p = np.histogram(learner_trajs_label)[0] / len(learner_trajs_label)\n",
    "    return distance.jensenshannon(test_p, learner_p)\n",
    "\n",
    "def compute_int(act_seq, n_time):\n",
    "    print(\"act_seq\", act_seq.shape)\n",
    "    act2int = np.zeros((11, n_time)) # count of intervals of different activities\n",
    "    for i in range(act_seq.shape[0]):\n",
    "        curr_act, curr_int = act_seq[i, 0], 1\n",
    "        for j in range(1, act_seq.shape[1]):\n",
    "            if act_seq[i, j] == curr_act:\n",
    "                curr_int += 1\n",
    "            else:\n",
    "                act2int[curr_act, curr_int - 1] = act2int[curr_act, curr_int - 1] + 1\n",
    "                curr_act, curr_int = act_seq[i, j], 1\n",
    "        act2int[curr_act, curr_int - 1] = act2int[curr_act, curr_int - 1] + 1\n",
    "    return act2int\n",
    "\n",
    "def macro_micro_int_jsd(gen_seq, tar_seq, n_time):\n",
    "    gen_act2int = compute_int(gen_seq, n_time)\n",
    "    tar_act2int = compute_int(tar_seq, n_time)\n",
    "    macro_int_jsd = distance.jensenshannon(np.sum(gen_act2int, 0) / np.sum(gen_act2int), np.sum(tar_act2int, 0) / np.sum(tar_act2int))\n",
    "    micro_int_jsd = distance.jensenshannon(gen_act2int.reshape(-1) / np.sum(gen_act2int), tar_act2int.reshape(-1) / np.sum(tar_act2int))\n",
    "    return macro_int_jsd, micro_int_jsd\n",
    "\n",
    "def compute_act_type(act_seq):\n",
    "    act2cnt = np.zeros(11)\n",
    "    for i in range(11):\n",
    "        act2cnt[i] = np.sum(act_seq == i)\n",
    "    return act2cnt\n",
    "\n",
    "def act_type_jsd(gen_seq, tar_seq):\n",
    "    gen_act2cnt = compute_act_type(gen_seq)\n",
    "    tar_act2cnt = compute_act_type(tar_seq)\n",
    "    type_jsd = distance.jensenshannon(gen_act2cnt / np.sum(gen_act2cnt), tar_act2cnt / np.sum(tar_act2cnt))\n",
    "    return type_jsd\n",
    "\n",
    "def compute_uni_act_type(act_seq):\n",
    "    act2cnt = np.zeros(11)\n",
    "    for i in range(act_seq.shape[0]):\n",
    "        curr_act = act_seq[i, 0]\n",
    "        act2cnt[curr_act] = act2cnt[curr_act] + 1\n",
    "        for j in range(1, act_seq.shape[1]):\n",
    "            if act_seq[i, j] == curr_act:\n",
    "                continue\n",
    "            else:\n",
    "                curr_act = act_seq[i, j]\n",
    "                act2cnt[curr_act] = act2cnt[curr_act] + 1\n",
    "    return act2cnt\n",
    "\n",
    "def uni_act_type_jsd(gen_seq, tar_seq):\n",
    "    gen_act2cnt = compute_uni_act_type(gen_seq)\n",
    "    tar_act2cnt = compute_uni_act_type(tar_seq)\n",
    "    type_jsd = distance.jensenshannon(gen_act2cnt / np.sum(gen_act2cnt), tar_act2cnt / np.sum(tar_act2cnt))\n",
    "    return type_jsd\n",
    "\n",
    "def compute_traj_len(act_seq):\n",
    "    traj_len_ls = []\n",
    "    for i in range(act_seq.shape[0]):\n",
    "        curr_act = act_seq[i, 0]\n",
    "        traj_len = 1\n",
    "        for j in range(1, act_seq.shape[1]):\n",
    "            if act_seq[i, j] == curr_act:\n",
    "                continue\n",
    "            else:\n",
    "                curr_act = act_seq[i, j]\n",
    "                traj_len += 1\n",
    "        traj_len_ls.append(traj_len)\n",
    "    traj_len_array = np.array(traj_len_ls)\n",
    "    traj_len_dist = np.zeros(np.max(traj_len_array))\n",
    "    for i in range(len(traj_len_dist)):\n",
    "        traj_len_dist[i] = np.sum(traj_len_array == i+1)\n",
    "    return traj_len_dist\n",
    "\n",
    "def traj_len_jsd(gen_seq, tar_seq):\n",
    "    gen_len_dist = compute_traj_len(gen_seq)\n",
    "    tar_len_dist = compute_traj_len(tar_seq)\n",
    "    if len(gen_len_dist) < len(tar_len_dist):\n",
    "        gen_len_dist = np.array(gen_len_dist.tolist() + [0] * (len(tar_len_dist) - len(gen_len_dist)))\n",
    "    elif len(tar_len_dist) < len(gen_len_dist):\n",
    "        tar_len_dist = np.array(tar_len_dist.tolist() + [0] * (len(gen_len_dist) - len(tar_len_dist)))\n",
    "    traj_len_jsd = distance.jensenshannon(gen_len_dist / np.sum(gen_len_dist), tar_len_dist / np.sum(tar_len_dist))\n",
    "    return traj_len_jsd\n",
    "\n",
    "def compute_hour(act_seq, n_time):\n",
    "    act2hour = np.zeros((11, n_time)) # count of intervals of different activities\n",
    "    for i in range(act_seq.shape[0]):\n",
    "        curr_act = act_seq[i, 0]\n",
    "        act2hour[curr_act, 0] = act2hour[curr_act, 0] + 1\n",
    "        for j in range(1, act_seq.shape[1]):\n",
    "            if act_seq[i, j] == curr_act:\n",
    "                continue\n",
    "            else:\n",
    "                curr_act = act_seq[i, j]\n",
    "                act2hour[curr_act, j] = act2hour[curr_act, j] + 1\n",
    "    return act2hour\n",
    "\n",
    "def macro_micro_hour_jsd(gen_seq, tar_seq, n_time):\n",
    "    gen_act2hour = compute_hour(gen_seq, n_time)\n",
    "    tar_act2hour = compute_hour(tar_seq, n_time)\n",
    "    macro_hour_jsd = distance.jensenshannon(np.sum(gen_act2hour, 0) / np.sum(gen_act2hour), np.sum(tar_act2hour, 0) / np.sum(tar_act2hour))\n",
    "    micro_hour_jsd = distance.jensenshannon(gen_act2hour.reshape(-1) / np.sum(gen_act2hour), tar_act2hour.reshape(-1) / np.sum(tar_act2hour))\n",
    "    return macro_hour_jsd, micro_hour_jsd\n",
    "\n",
    "def generated_tuple2seq(gen_tuples):\n",
    "    gen_trajs = [[user_gen_tuple[0] for user_gen_tuple in user_gen_tuples] for user_gen_tuples in gen_tuples]\n",
    "    return np.array(gen_trajs)\n",
    "\n",
    "def evaluation(gen_seq, tar_seq, n_time):\n",
    "    macro_int_jsd, micro_int_jsd = macro_micro_int_jsd(gen_seq, tar_seq, n_time)\n",
    "    macro_hour_jsd, micro_hour_jsd = macro_micro_hour_jsd(gen_seq, tar_seq, n_time)\n",
    "    results = {\"accuracy\": acc(gen_seq, tar_seq),\n",
    "               \"f1-score\": f1(gen_seq, tar_seq),\n",
    "               \"edit_dist\": edit_dist(gen_seq, tar_seq),\n",
    "               \"bleu_score\": bleu_score(gen_seq, tar_seq),\n",
    "               \"data_jsd\": dataset_jsd(gen_seq, tar_seq),\n",
    "               \"macro_int\": macro_int_jsd,\n",
    "               \"micro_int\": micro_int_jsd,\n",
    "               \"act_type\": act_type_jsd(gen_seq, tar_seq),\n",
    "               \"uni_act_type\": uni_act_type_jsd(gen_seq, tar_seq),\n",
    "               \"traj_len\": traj_len_jsd(gen_seq, tar_seq),\n",
    "               \"macro_hour\": macro_hour_jsd,\n",
    "               \"micro_hour\": micro_hour_jsd}\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_log_prob(policy, test_trajs, batch_ind_feat, batch_ind_emp):\n",
    "    log_prob_ls = []\n",
    "    for i in range(batch_ind_feat.shape[0]):\n",
    "        activity, time, dur, traj_len, dur_leave_home, dur_travel, ind_feat, ind_emp = env.reset(batch_ind_feat[i], batch_ind_emp[i])\n",
    "        ind_feat_var = torch.tensor(ind_feat).float().unsqueeze(0).to(device)\n",
    "        ind_emp_var = torch.tensor(ind_emp).long().unsqueeze(0).to(device)\n",
    "        # get_log_prob(self, curr_activity, curr_tim, curr_dur, curr_traj_len, curr_dur_leave_home, curr_dur_travel, ind_feat, ind_emp, actions):\n",
    "        seq_log_prob = 0\n",
    "        for t in range(env.time_size):\n",
    "            activity_var = torch.tensor(activity).long().unsqueeze(0).to(device)\n",
    "            time_var = torch.tensor(time).long().unsqueeze(0).to(device)\n",
    "            dur_var = torch.tensor(dur).long().unsqueeze(0).to(device)\n",
    "            traj_len_var = torch.tensor(traj_len).long().unsqueeze(0).to(device)\n",
    "            dur_leave_home_var = torch.tensor(dur_leave_home).long().unsqueeze(0).to(device)\n",
    "            dur_travel_var = torch.tensor(dur_travel).long().unsqueeze(0).to(device)\n",
    "            next_activity = test_trajs[i][t]\n",
    "            next_activity_var = torch.tensor(next_activity).long().unsqueeze(0).to(device)\n",
    "            with torch.no_grad():\n",
    "                log_prob = policy.get_log_prob(activity_var, time_var, dur_var, traj_len_var, \\\n",
    "                                dur_leave_home_var, dur_travel_var, ind_feat_var, ind_emp_var, next_activity_var)\n",
    "            seq_log_prob += log_prob.item()\n",
    "            next_time, next_dur, next_traj_len, next_dur_leave_home, next_dur_travel, action, _, done = \\\n",
    "                env.step(activity, time, dur, traj_len, dur_leave_home, dur_travel, next_activity)\n",
    "            if done:\n",
    "                break\n",
    "            activity, time, dur, traj_len, dur_leave_home, dur_travel = next_activity, next_time, next_dur, next_traj_len, next_dur_leave_home, next_dur_travel\n",
    "        log_prob_ls.append(seq_log_prob)\n",
    "    return np.mean(log_prob_ls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iT4hsl7sd3xg"
   },
   "source": [
    "TORCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Yk7Xy8DLd3Ig"
   },
   "outputs": [],
   "source": [
    "def to_device(device, *args):\n",
    "    return [x.to(device) for x in args]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "254JHsHdcTL_"
   },
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wSJPOvoVj7r7"
   },
   "source": [
    "DISTRIMINATOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "8dBGBIxa5HHw"
   },
   "outputs": [],
   "source": [
    "class DiscriminatorAIRL(nn.Module):\n",
    "    def __init__(self, activity_size, activity_emb_size,\n",
    "            tim_size, tim_emb_size,\n",
    "            dur_size, dur_emb_size,\n",
    "            traj_len_size, traj_len_emb_size,\n",
    "            dur_leave_home_size, dur_leave_home_emb_size,\n",
    "            dur_travel_size, dur_travel_emb_size,\n",
    "            act_size, act_emb_size,\n",
    "            emp_num_size,\n",
    "            emp_size, emp_emb_size,\n",
    "            dropout_p=0.5, gamma=0.99):\n",
    "        super(DiscriminatorAIRL, self).__init__()\n",
    "\n",
    "        # self.device = torch.device('cuda:4') if torch.cuda.is_available() else torch.device('cpu')\n",
    "        self.gamma = gamma\n",
    "        self.activity_size = activity_size\n",
    "        self.activity_emb_size = activity_emb_size\n",
    "        self.tim_size = tim_size\n",
    "        self.tim_emb_size = tim_emb_size\n",
    "        self.dur_size = dur_size\n",
    "        self.dur_emb_size = dur_emb_size\n",
    "        self.traj_len_size = traj_len_size\n",
    "        self.traj_len_emb_size = traj_len_emb_size\n",
    "        self.dur_leave_home_size = dur_leave_home_size\n",
    "        self.dur_leave_home_emb_size = dur_leave_home_emb_size\n",
    "        self.dur_travel_size = dur_travel_size\n",
    "        self.dur_travel_emb_size = dur_travel_emb_size\n",
    "        self.act_size = act_size\n",
    "        self.act_emb_size = act_emb_size\n",
    "        self.emp_size = emp_size\n",
    "        self.emp_emb_size = emp_emb_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.emb_activity = nn.Embedding(self.activity_size + 1, self.activity_emb_size)\n",
    "        self.emb_tim = nn.Embedding(self.tim_size + 1, self.tim_emb_size)\n",
    "        self.emb_dur = nn.Embedding(self.dur_size + 1, self.dur_emb_size)\n",
    "        self.emb_traj_len = nn.Embedding(self.traj_len_size + 1, self.traj_len_emb_size)\n",
    "        self.emb_dur_leave_home = nn.Embedding(self.dur_leave_home_size + 1, self.dur_leave_home_emb_size)\n",
    "        self.emb_dur_travel = nn.Embedding(self.dur_travel_size + 1, self.dur_travel_emb_size)\n",
    "        self.emb_act = nn.Embedding(self.act_size, self.act_emb_size)\n",
    "        self.emb_emp = nn.Embedding(self.emp_size, self.emp_emb_size)\n",
    "\n",
    "        state_size = self.activity_emb_size + self.tim_emb_size  + self.dur_emb_size + \\\n",
    "                self.traj_len_emb_size + self.dur_leave_home_emb_size + self.dur_travel_emb_size\n",
    "        act_size = self.act_emb_size\n",
    "        ind_feat_size = self.emp_emb_size + emp_num_size\n",
    "        self.fc1 = nn.Linear(state_size + act_size + ind_feat_size, 120)  # [batch, 120]\n",
    "        self.fc2 = nn.Linear(120, 84)  # [batch, 84]\n",
    "        self.fc3 = nn.Linear(84, 1)  # [batch, 8]\n",
    "\n",
    "        self.h_fc1 = nn.Linear(state_size + ind_feat_size, 120)  # [batch, 120]\n",
    "        self.h_fc2 = nn.Linear(120, 84)  # [batch, 84]\n",
    "        self.h_fc3 = nn.Linear(84, 1)  # [batch, 8]\n",
    "\n",
    "    def f(self, curr_activity, curr_tim, curr_dur, curr_traj_len, curr_dur_leave_home, curr_dur_travel, act,\n",
    "          next_activity, next_tim, next_dur, next_traj_len, next_dur_leave_home, next_dur_travel,\n",
    "          ind_feat, ind_emp):\n",
    "        # 我在想是不是可以 rs 和之前一样 因为rs 给我感觉更加像从s 到 s' 通过a的instant reward\n",
    "        \"\"\"rs\"\"\"\n",
    "        curr_activity_emb = self.emb_activity(curr_activity) # batch_size, n_len, n_emb\n",
    "        curr_tim_emb = self.emb_tim(curr_tim)\n",
    "        curr_dur_emb = self.emb_dur(curr_dur)\n",
    "        curr_traj_len_emb = self.emb_traj_len(curr_traj_len)\n",
    "        curr_dur_leave_home_emb = self.emb_dur_leave_home(curr_dur_leave_home)\n",
    "        curr_dur_travel_emb = self.emb_dur_travel(curr_dur_travel)\n",
    "        x_state = torch.cat([curr_activity_emb, curr_tim_emb, curr_dur_emb, \\\n",
    "                      curr_traj_len_emb, curr_dur_leave_home_emb, curr_dur_travel_emb], -1)\n",
    "        x_act = self.emb_act(act)\n",
    "        ind_emp_emb = self.emb_emp(ind_emp)\n",
    "        ind_feat = torch.cat([ind_feat, ind_emp_emb], -1)\n",
    "        x_rs = torch.cat([x_state, x_act, ind_feat], -1)\n",
    "        x_rs = F.leaky_relu(self.fc1(x_rs), 0.2)\n",
    "        x_rs = F.leaky_relu(self.fc2(x_rs), 0.2)  # 我个人的建议是你先把它按照图像处理完\n",
    "        x_rs = self.fc3(x_rs)\n",
    "\n",
    "        \"\"\"hs\"\"\" # cur_act, cur_tim, stay_dur\n",
    "        x_hs = torch.cat([x_state, ind_feat], -1)\n",
    "        x_hs = F.leaky_relu(self.h_fc1(x_hs), 0.2)\n",
    "        x_hs = F.leaky_relu(self.h_fc2(x_hs), 0.2)\n",
    "        x_hs = self.h_fc3(x_hs)\n",
    "\n",
    "        \"\"\"hs_next\"\"\" # next_act, next_tim, next_dur\n",
    "        next_activity_emb = self.emb_activity(next_activity) # batch_size, n_len, n_emb\n",
    "        next_tim_emb = self.emb_tim(next_tim)\n",
    "        next_dur_emb = self.emb_dur(next_dur)\n",
    "        next_traj_len_emb = self.emb_traj_len(next_traj_len)\n",
    "        next_dur_leave_home_emb = self.emb_dur_leave_home(next_dur_leave_home)\n",
    "        next_dur_travel_emb = self.emb_dur_travel(next_dur_travel)\n",
    "        next_x_state = torch.cat([next_activity_emb, next_tim_emb, next_dur_emb, \\\n",
    "                          next_traj_len_emb, next_dur_leave_home_emb, next_dur_travel_emb], -1)\n",
    "        x_hs_next = torch.cat([next_x_state, ind_feat], -1)\n",
    "        x_hs_next = F.leaky_relu(self.h_fc1(x_hs_next), 0.2)\n",
    "        x_hs_next = F.leaky_relu(self.h_fc2(x_hs_next), 0.2)\n",
    "        x_hs_next = self.h_fc3(x_hs_next)\n",
    "\n",
    "        return x_rs + self.gamma * x_hs_next - x_hs\n",
    "\n",
    "    def forward(self, curr_activity, curr_tim, curr_dur, curr_traj_len, curr_dur_leave_home, curr_dur_travel, act, \\\n",
    "                next_activity, next_tim, next_dur, next_traj_len, next_dur_leave_home, next_dur_travel, \\\n",
    "                ind_feat, ind_emp, log_pis):\n",
    "        # Discriminator's output is sigmoid(f - log_pi).\n",
    "        return self.f(curr_activity, curr_tim, curr_dur, curr_traj_len, curr_dur_leave_home, curr_dur_travel, act, \\\n",
    "                      next_activity, next_tim, next_dur, next_traj_len, next_dur_leave_home, next_dur_travel, \\\n",
    "                      ind_feat, ind_emp) - log_pis\n",
    "\n",
    "    def calculate_reward(self, curr_activity, curr_tim, curr_dur, curr_traj_len, curr_dur_leave_home, curr_dur_travel, act, \\\n",
    "                next_activity, next_tim, next_dur, next_traj_len, next_dur_leave_home, next_dur_travel, \\\n",
    "                ind_feat, ind_emp, log_pis):\n",
    "        with torch.no_grad():\n",
    "            logits = self.forward(curr_activity, curr_tim, curr_dur, curr_traj_len, curr_dur_leave_home, curr_dur_travel, act, \\\n",
    "                      next_activity, next_tim, next_dur, next_traj_len, next_dur_leave_home, next_dur_travel, \\\n",
    "                      ind_feat, ind_emp, log_pis)\n",
    "            return -F.logsigmoid(-logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fe13EtLRj_Tz"
   },
   "source": [
    "POLICY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "NbxfbwzjMRK-"
   },
   "outputs": [],
   "source": [
    "class PolicyNet(nn.Module):\n",
    "    def __init__(self, activity_size, activity_emb_size,\n",
    "            tim_size, tim_emb_size,\n",
    "            dur_size, dur_emb_size,\n",
    "            traj_len_size, traj_len_emb_size,\n",
    "            dur_leave_home_size, dur_leave_home_emb_size,\n",
    "            dur_travel_size, dur_travel_emb_size,\n",
    "            emp_num_size,\n",
    "            emp_size, emp_emb_size,\n",
    "            dropout_p=0.5, gamma=0.99):\n",
    "        super(PolicyNet, self).__init__()\n",
    "\n",
    "        # self.device = torch.device('cuda:4') if torch.cuda.is_available() else torch.device('cpu')\n",
    "        self.gamma = gamma\n",
    "        self.activity_size = activity_size\n",
    "        self.activity_emb_size = activity_emb_size\n",
    "        self.tim_size = tim_size\n",
    "        self.tim_emb_size = tim_emb_size\n",
    "        self.dur_size = dur_size\n",
    "        self.dur_emb_size = dur_emb_size\n",
    "        self.traj_len_size = traj_len_size\n",
    "        self.traj_len_emb_size = traj_len_emb_size\n",
    "        self.dur_leave_home_size = dur_leave_home_size\n",
    "        self.dur_leave_home_emb_size = dur_leave_home_emb_size\n",
    "        self.dur_travel_size = dur_travel_size\n",
    "        self.dur_travel_emb_size = dur_travel_emb_size\n",
    "        self.emp_size = emp_size\n",
    "        self.emp_emb_size = emp_emb_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.emb_activity = nn.Embedding(self.activity_size + 1, self.activity_emb_size)\n",
    "        self.emb_tim = nn.Embedding(self.tim_size + 1, self.tim_emb_size)\n",
    "        self.emb_dur = nn.Embedding(self.dur_size + 1, self.dur_emb_size)\n",
    "        self.emb_traj_len = nn.Embedding(self.traj_len_size + 1, self.traj_len_emb_size)\n",
    "        self.emb_dur_leave_home = nn.Embedding(self.dur_leave_home_size + 1, self.dur_leave_home_emb_size)\n",
    "        self.emb_dur_travel = nn.Embedding(self.dur_travel_size + 1, self.dur_travel_emb_size)\n",
    "        self.emb_emp = nn.Embedding(self.emp_size, self.emp_emb_size)\n",
    "\n",
    "        state_size = self.activity_emb_size + self.tim_emb_size  + self.dur_emb_size + \\\n",
    "                self.traj_len_emb_size + self.dur_leave_home_emb_size + self.dur_travel_emb_size\n",
    "        ind_feat_size = self.emp_emb_size + emp_num_size\n",
    "        self.fc1 = nn.Linear(state_size + ind_feat_size, 120)  # [batch, 120]\n",
    "        self.fc2 = nn.Linear(120, 84)  # [batch, 84]\n",
    "        self.fc3 = nn.Linear(84, self.activity_size)  # [batch, 8]\n",
    "\n",
    "    def forward(self, curr_activity, curr_tim, curr_dur, curr_traj_len, curr_dur_leave_home, curr_dur_travel, ind_feat, ind_emp):  # 这是policy\n",
    "        # print(\"policy networks\", curr_activity, curr_tim, curr_dur, \"tim_size\", self.tim_size)\n",
    "        curr_activity_emb = self.emb_activity(curr_activity) # batch_size, n_len, n_emb\n",
    "        curr_tim_emb = self.emb_tim(curr_tim)\n",
    "        curr_dur_emb = self.emb_dur(curr_dur)\n",
    "        curr_traj_len_emb = self.emb_traj_len(curr_traj_len)\n",
    "        curr_dur_leave_home_emb = self.emb_dur_leave_home(curr_dur_leave_home)\n",
    "        curr_dur_travel_emb = self.emb_dur_travel(curr_dur_travel)\n",
    "        x_state = torch.cat([curr_activity_emb, curr_tim_emb, curr_dur_emb, \\\n",
    "                      curr_traj_len_emb, curr_dur_leave_home_emb, curr_dur_travel_emb], -1)\n",
    "        ind_emp_emb = self.emb_emp(ind_emp)\n",
    "        ind_feat = torch.cat([ind_feat, ind_emp_emb], -1)\n",
    "        x = torch.cat([x_state, ind_feat], -1)\n",
    "        x = F.leaky_relu(self.fc1(x), 0.2)\n",
    "        x = F.leaky_relu(self.fc2(x), 0.2)\n",
    "        x = self.fc3(x)\n",
    "        # if torch.isnan(x).sum() > 0:\n",
    "        #   print({\"curr_activity\": curr_activity[0].item(), \"curr_tim\": curr_tim[0].item(), \"curr_dur\": curr_dur[0].item()})\n",
    "        #   print(\"whether x_state is nan\", torch.isnan(x_state).sum())\n",
    "        #   print(\"whether ind_feat is nan\", torch.isnan(ind_feat).sum())\n",
    "        return x\n",
    "\n",
    "    def get_action_prob(self, curr_activity, curr_tim, curr_dur, curr_traj_len, curr_dur_leave_home, curr_dur_travel, ind_feat, ind_emp):\n",
    "        x = self.forward(curr_activity, curr_tim, curr_dur, curr_traj_len, curr_dur_leave_home, curr_dur_travel, ind_feat, ind_emp)\n",
    "        \"\"\"if curr_activity is pad, the next activity should be stay_activity\"\"\"\n",
    "        pad_mask = curr_activity.unsqueeze(1).repeat(1, self.activity_size) == self.activity_size # batch_size, self.activity_size\n",
    "        pad_mask = torch.cat([torch.zeros((curr_activity.shape[0], self.activity_size-1)).float().to(curr_activity.device), pad_mask[:, -1:]], -1)\n",
    "        x = x.masked_fill(pad_mask.bool(), -1e32)\n",
    "        x_mask = curr_activity.unsqueeze(1).repeat(1, self.activity_size) < (self.activity_size - 1)\n",
    "        stay_mask = torch.cat([F.one_hot(curr_activity, num_classes=self.activity_size + 1)[:, :-2],\n",
    "                               torch.ones((curr_activity.shape[0], 1)).float().to(curr_activity.device)], -1)\n",
    "        x_mask = x_mask * (1 - stay_mask)\n",
    "        x = x.masked_fill(x_mask.bool(), -1e32)\n",
    "        prob_x = F.softmax(x, dim=-1)\n",
    "        # if torch.isnan(prob_x).sum() > 0:\n",
    "        # if (prob_x.sum(axis=1) < 0.999).sum() > 0:\n",
    "        #   print({\"curr_activity\": curr_activity[0].item(), \"curr_tim\": curr_tim[0].item(), \"curr_dur\": curr_dur[0].item(), \"prob_x\": prob_x[0], \"x\": x[0]})\n",
    "        return prob_x\n",
    "\n",
    "    def select_action(self, curr_activity, curr_tim, curr_dur, curr_traj_len, curr_dur_leave_home, curr_dur_travel, ind_feat, ind_emp):\n",
    "        action_prob = self.get_action_prob(curr_activity, curr_tim, curr_dur, curr_traj_len, curr_dur_leave_home, curr_dur_travel, ind_feat, ind_emp)\n",
    "        action = torch.distributions.Categorical(action_prob).sample()\n",
    "        return action\n",
    "\n",
    "    def get_kl(self, curr_activity, curr_tim, curr_dur, curr_traj_len, curr_dur_leave_home, curr_dur_travel, ind_feat, ind_emp):\n",
    "        action_prob1 = self.get_action_prob(curr_activity, curr_tim, curr_dur, curr_traj_len, curr_dur_leave_home, curr_dur_travel, ind_feat, ind_emp)\n",
    "        action_prob0 = action_prob1.detach()\n",
    "        kl = action_prob0 * (torch.log(action_prob0) - torch.log(action_prob1))\n",
    "        return kl.sum(1, keepdim=True)\n",
    "\n",
    "    def get_log_prob(self, curr_activity, curr_tim, curr_dur, curr_traj_len, curr_dur_leave_home, curr_dur_travel, ind_feat, ind_emp, actions):\n",
    "        # 我感觉问题在于 有一些action的prob是接近0的\n",
    "        action_prob = self.get_action_prob(curr_activity, curr_tim, curr_dur, curr_traj_len, curr_dur_leave_home, curr_dur_travel, ind_feat, ind_emp)\n",
    "        next_activity_prob = action_prob.gather(1, actions.long().unsqueeze(1))\n",
    "        # exam_next_activity_prob = next_activity_prob.squeeze(-1)\n",
    "        # if (exam_next_activity_prob < 0.001).sum() > 0:\n",
    "        #   print(\"curr_activity\", curr_activity.shape, \"actions\", actions.shape, \"next_activity_prob\", exam_next_activity_prob.shape)\n",
    "        #   print(\"================================impossible action choice\")\n",
    "        #   print(\"curr_activity\", curr_activity[exam_next_activity_prob < 0.001])\n",
    "        #   print(\"curr_tim\", curr_tim[exam_next_activity_prob < 0.001])\n",
    "        #   print(\"curr_dur\", curr_dur[exam_next_activity_prob < 0.001])\n",
    "        #   print(\"next_activities\", actions[exam_next_activity_prob < 0.001])\n",
    "        #   print(\"next_activity_prob\", exam_next_activity_prob[exam_next_activity_prob < 0.001])\n",
    "        return torch.log(action_prob.gather(1, actions.long().unsqueeze(1)))\n",
    "\n",
    "    def get_fim(self, curr_activity, curr_tim, curr_dur, curr_traj_len, curr_dur_leave_home, curr_dur_travel, ind_feat, ind_emp):\n",
    "        action_prob = self.get_action_prob(curr_activity, curr_tim, curr_dur, curr_traj_len, curr_dur_leave_home, curr_dur_travel, ind_feat, ind_emp)\n",
    "        M = action_prob.pow(-1).view(-1).detach()\n",
    "        return M, action_prob, {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xlZdasubfUjZ"
   },
   "source": [
    "Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "5CI_jmLFfTOZ"
   },
   "outputs": [],
   "source": [
    "class ValueNet(nn.Module):\n",
    "    def __init__(self, activity_size, activity_emb_size,\n",
    "            tim_size, tim_emb_size,\n",
    "            dur_size, dur_emb_size,\n",
    "            traj_len_size, traj_len_emb_size,\n",
    "            dur_leave_home_size, dur_leave_home_emb_size,\n",
    "            dur_travel_size, dur_travel_emb_size,\n",
    "            emp_num_size,\n",
    "            emp_size, emp_emb_size,\n",
    "            dropout_p=0.5, gamma=0.99):\n",
    "        super(ValueNet, self).__init__()\n",
    "\n",
    "        # self.device = torch.device('cuda:4') if torch.cuda.is_available() else torch.device('cpu')\n",
    "        self.gamma = gamma\n",
    "        self.activity_size = activity_size\n",
    "        self.activity_emb_size = activity_emb_size\n",
    "        self.tim_size = tim_size\n",
    "        self.tim_emb_size = tim_emb_size\n",
    "        self.dur_size = dur_size\n",
    "        self.dur_emb_size = dur_emb_size\n",
    "        self.traj_len_size = traj_len_size\n",
    "        self.traj_len_emb_size = traj_len_emb_size\n",
    "        self.dur_leave_home_size = dur_leave_home_size\n",
    "        self.dur_leave_home_emb_size = dur_leave_home_emb_size\n",
    "        self.dur_travel_size = dur_travel_size\n",
    "        self.dur_travel_emb_size = dur_travel_emb_size\n",
    "        self.emp_size = emp_size\n",
    "        self.emp_emb_size = emp_emb_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.emb_activity = nn.Embedding(self.activity_size + 1, self.activity_emb_size)\n",
    "        self.emb_tim = nn.Embedding(self.tim_size + 1, self.tim_emb_size)\n",
    "        self.emb_dur = nn.Embedding(self.dur_size + 1, self.dur_emb_size)\n",
    "        self.emb_traj_len = nn.Embedding(self.traj_len_size + 1, self.traj_len_emb_size)\n",
    "        self.emb_dur_leave_home = nn.Embedding(self.dur_leave_home_size + 1, self.dur_leave_home_emb_size)\n",
    "        self.emb_dur_travel = nn.Embedding(self.dur_travel_size + 1, self.dur_travel_emb_size)\n",
    "        self.emb_emp = nn.Embedding(self.emp_size, self.emp_emb_size)\n",
    "\n",
    "        state_size = self.activity_emb_size + self.tim_emb_size  + self.dur_emb_size + \\\n",
    "                self.traj_len_emb_size + self.dur_leave_home_emb_size + self.dur_travel_emb_size\n",
    "        ind_feat_size = self.emp_emb_size + emp_num_size\n",
    "        self.fc1 = nn.Linear(state_size + ind_feat_size, 120)  # [batch, 120]\n",
    "        self.fc2 = nn.Linear(120, 84)  # [batch, 84]\n",
    "        self.fc3 = nn.Linear(84, 1)  # [batch, 8]\n",
    "\n",
    "    def forward(self, curr_activity, curr_tim, curr_dur, curr_traj_len, curr_dur_leave_home, curr_dur_travel, ind_feat, ind_emp):  # 这是policy\n",
    "        curr_activity_emb = self.emb_activity(curr_activity) # batch_size, n_len, n_emb\n",
    "        curr_tim_emb = self.emb_tim(curr_tim)\n",
    "        curr_dur_emb = self.emb_dur(curr_dur)\n",
    "        curr_traj_len_emb = self.emb_traj_len(curr_traj_len)\n",
    "        curr_dur_leave_home_emb = self.emb_dur_leave_home(curr_dur_leave_home)\n",
    "        curr_dur_travel_emb = self.emb_dur_travel(curr_dur_travel)\n",
    "        x_state = torch.cat([curr_activity_emb, curr_tim_emb, curr_dur_emb, \\\n",
    "                      curr_traj_len_emb, curr_dur_leave_home_emb, curr_dur_travel_emb], -1)\n",
    "        ind_emp_emb = self.emb_emp(ind_emp)\n",
    "        ind_feat = torch.cat([ind_feat, ind_emp_emb], -1)\n",
    "        x = torch.cat([x_state, ind_feat], -1)\n",
    "        x = F.leaky_relu(self.fc1(x), 0.2)\n",
    "        x = F.leaky_relu(self.fc2(x), 0.2)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3OQJTST0cuiW"
   },
   "source": [
    "## Core"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QgIE_LLedyf5"
   },
   "source": [
    "REPLAY MEMORY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "Y8Aodcyud0TZ"
   },
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "import random\n",
    "\n",
    "# Taken from\n",
    "# https://github.com/pytorch/tutorials/blob/master/Reinforcement%20(Q-)Learning%20with%20PyTorch.ipynb\n",
    "\n",
    "Transition = namedtuple('Transition', ('activity', 'time', 'dur', 'traj_len', 'dur_leave_home', 'dur_travel', 'action', \\\n",
    "                          'next_activity', 'next_time', 'next_dur', 'next_traj_len', 'next_dur_leave_home', 'next_dur_travel', \\\n",
    "                          'reward', 'mask', 'ind_feat', 'ind_emp'))\n",
    "\n",
    "# memory.push(activity, time, dur, action, next_activity, next_time, next_dur, reward, mask, ind_feat, ind_emp)\n",
    "\n",
    "class Memory(object):\n",
    "    def __init__(self):\n",
    "        self.memory = []\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Saves a transition.\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size=None):\n",
    "        if batch_size is None:\n",
    "            return Transition(*zip(*self.memory))\n",
    "        else:\n",
    "            random_batch = random.sample(self.memory, batch_size)\n",
    "            return Transition(*zip(*random_batch))\n",
    "\n",
    "    def append(self, new_memory):\n",
    "        self.memory += new_memory.memory\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OqXHc1WudTZw"
   },
   "source": [
    "AGENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RP4qZllkc1eg"
   },
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "import math\n",
    "import time\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "\n",
    "\n",
    "def collect_samples(pid, queue, env, policy, custom_reward,\n",
    "                    mean_action, render, running_state, min_batch_size):\n",
    "    if pid > 0:\n",
    "        torch.manual_seed(torch.randint(0, 5000, (1,)) * pid)\n",
    "        if hasattr(env, 'np_random'):\n",
    "            env.np_random.seed(env.np_random.randint(5000) * pid)\n",
    "        if hasattr(env, 'env') and hasattr(env.env, 'np_random'):\n",
    "            env.env.np_random.seed(env.env.np_random.randint(5000) * pid)\n",
    "    log = dict()\n",
    "    memory = Memory()\n",
    "    num_steps = 0\n",
    "    total_reward = 0\n",
    "    min_reward = 1e6\n",
    "    max_reward = -1e6\n",
    "    total_c_reward = 0\n",
    "    min_c_reward = 1e6\n",
    "    max_c_reward = -1e6\n",
    "    num_episodes = 0\n",
    "\n",
    "    while num_steps < min_batch_size:\n",
    "        activity, time, dur, traj_len, dur_leave_home, dur_travel, ind_feat, ind_emp = env.reset()  # 我自己有点担心 randomly select 可能会让它区分出来 我自己是希望尽量用差不多的state和des 但是先这样吧 先跑通\n",
    "        reward_episode = 0\n",
    "        ind_feat_var = torch.tensor(ind_feat).float().unsqueeze(0)\n",
    "        ind_emp_var = torch.tensor(ind_emp).long().unsqueeze(0)\n",
    "        for t in range(env.time_size):\n",
    "            activity_var = torch.tensor(activity).long().unsqueeze(0)\n",
    "            time_var = torch.tensor(time).long().unsqueeze(0)\n",
    "            dur_var = torch.tensor(dur).long().unsqueeze(0)\n",
    "            traj_len_var = torch.tensor(traj_len).long().unsqueeze(0)\n",
    "            dur_leave_home_var = torch.tensor(dur_leave_home).long().unsqueeze(0)\n",
    "            dur_travel_var = torch.tensor(dur_travel).long().unsqueeze(0)\n",
    "            with torch.no_grad():\n",
    "                if mean_action:\n",
    "                    next_activity = torch.argmax(policy.get_action_prob(activity_var, time_var, dur_var, traj_len_var, \\\n",
    "                                    dur_leave_home_var, dur_travel_var, ind_feat_var, ind_emp_var)).unsqueeze(0).numpy()\n",
    "                else:\n",
    "                    next_activity = policy.select_action(activity_var, time_var, dur_var, traj_len_var, \\\n",
    "                                    dur_leave_home_var, dur_travel_var, ind_feat_var, ind_emp_var)[0].numpy()\n",
    "            next_activity = int(next_activity)\n",
    "            next_time, next_dur, next_traj_len, next_dur_leave_home, next_dur_travel, action, reward, done = \\\n",
    "                env.step(activity, time, dur, traj_len, dur_leave_home, dur_travel, next_activity)\n",
    "            reward_episode += reward\n",
    "            mask = 0 if (done or t == env.time_size - 1) else 1\n",
    "            memory.push(activity, time, dur, traj_len, dur_leave_home, dur_travel, action, \\\n",
    "                        next_activity, next_time, next_dur, next_traj_len, next_dur_leave_home, next_dur_travel, \\\n",
    "                        reward, mask, ind_feat, ind_emp)\n",
    "            if done:\n",
    "                break\n",
    "            activity, time, dur, traj_len, dur_leave_home, dur_travel = next_activity, next_time, next_dur, \\\n",
    "                        next_traj_len, next_dur_leave_home, next_dur_travel\n",
    "        num_steps += (t + 1)\n",
    "        num_episodes += 1\n",
    "        total_reward += reward_episode \n",
    "        min_reward = min(min_reward, reward_episode)\n",
    "        max_reward = max(max_reward, reward_episode)\n",
    "\n",
    "    log['num_steps'] = num_steps\n",
    "    log['num_episodes'] = num_episodes\n",
    "    log['total_reward'] = total_reward\n",
    "    log['avg_reward'] = total_reward / num_episodes\n",
    "    log['max_reward'] = max_reward\n",
    "    log['min_reward'] = min_reward\n",
    "\n",
    "    if queue is not None:\n",
    "        queue.put([pid, memory, log])\n",
    "    else:\n",
    "        return memory, log\n",
    "\n",
    "\n",
    "def collect_test_trajs(pid, batch_ind_feat, batch_ind_emp, queue, env, policy, custom_reward,\n",
    "              mean_action, render, running_state):\n",
    "    if pid > 0:\n",
    "        torch.manual_seed(torch.randint(0, 5000, (1,)) * pid)\n",
    "        if hasattr(env, 'np_random'):\n",
    "            env.np_random.seed(env.np_random.randint(5000) * pid)\n",
    "        if hasattr(env, 'env') and hasattr(env.env, 'np_random'):\n",
    "            env.env.np_random.seed(env.env.np_random.randint(5000) * pid)\n",
    "\n",
    "    trajs = []\n",
    "    for i in range(batch_ind_feat.shape[0]):\n",
    "        activity, time, dur, traj_len, dur_leave_home, dur_travel, ind_feat, ind_emp = env.reset(batch_ind_feat[i], batch_ind_emp[i])\n",
    "        reward_episode = 0\n",
    "        ind_feat_var = torch.tensor(ind_feat).float().unsqueeze(0)\n",
    "        ind_emp_var = torch.tensor(ind_emp).long().unsqueeze(0)\n",
    "        traj = []\n",
    "        for t in range(env.time_size):  # 这个感觉是maximum_length的意思\n",
    "            activity_var = torch.tensor(activity).long().unsqueeze(0)\n",
    "            time_var = torch.tensor(time).long().unsqueeze(0)\n",
    "            dur_var = torch.tensor(dur).long().unsqueeze(0)\n",
    "            traj_len_var = torch.tensor(traj_len).long().unsqueeze(0)\n",
    "            dur_leave_home_var = torch.tensor(dur_leave_home).long().unsqueeze(0)\n",
    "            dur_travel_var = torch.tensor(dur_travel).long().unsqueeze(0)\n",
    "            with torch.no_grad():\n",
    "                if mean_action:\n",
    "                    next_activity = torch.argmax(policy.get_action_prob(activity_var, time_var, dur_var, traj_len_var, \\\n",
    "                                    dur_leave_home_var, dur_travel_var, ind_feat_var, ind_emp_var)).unsqueeze(0).numpy()\n",
    "                else:\n",
    "                    next_activity = policy.select_action(activity_var, time_var, dur_var, traj_len_var, \\\n",
    "                                    dur_leave_home_var, dur_travel_var, ind_feat_var, ind_emp_var)[0].numpy()\n",
    "            next_activity = int(next_activity)\n",
    "            next_time, next_dur, next_traj_len, next_dur_leave_home, next_dur_travel, action, _, done = \\\n",
    "                env.step(activity, time, dur, traj_len, dur_leave_home, dur_travel, next_activity)\n",
    "            traj.append((next_activity, next_time, next_dur))\n",
    "            if done:\n",
    "                break\n",
    "            activity, time, dur, traj_len, dur_leave_home, dur_travel = next_activity, next_time, next_dur, next_traj_len, next_dur_leave_home, next_dur_travel\n",
    "        trajs.append(traj)\n",
    "    if queue is not None:\n",
    "        queue.put([pid, trajs])\n",
    "    else:\n",
    "        return trajs\n",
    "\n",
    "\n",
    "def merge_log(log_list):\n",
    "    log = dict()\n",
    "    log['total_reward'] = sum([x['total_reward'] for x in log_list])\n",
    "    log['num_episodes'] = sum([x['num_episodes'] for x in log_list])\n",
    "    log['num_steps'] = sum([x['num_steps'] for x in log_list])\n",
    "    log['avg_reward'] = log['total_reward'] / log['num_episodes']\n",
    "    log['max_reward'] = max([x['max_reward'] for x in log_list])\n",
    "    log['min_reward'] = min([x['min_reward'] for x in log_list])\n",
    "    if 'total_c_reward' in log_list[0]:\n",
    "        log['total_c_reward'] = sum([x['total_c_reward'] for x in log_list])\n",
    "        log['avg_c_reward'] = log['total_c_reward'] / log['num_steps']\n",
    "        log['max_c_reward'] = max([x['max_c_reward'] for x in log_list])\n",
    "        log['min_c_reward'] = min([x['min_c_reward'] for x in log_list])\n",
    "\n",
    "    return log\n",
    "\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, env, policy, device, custom_reward=None, running_state=None, num_threads=1):\n",
    "        self.env = env\n",
    "        self.policy = policy\n",
    "        self.device = device\n",
    "        self.custom_reward = custom_reward\n",
    "        self.running_state = running_state\n",
    "        self.num_threads = num_threads\n",
    "\n",
    "    def collect_samples(self, min_batch_size, mean_action=False, render=False):\n",
    "        t_start = time.time()\n",
    "        to_device(torch.device('cpu'), self.policy)\n",
    "        thread_batch_size = int(math.floor(min_batch_size / self.num_threads))\n",
    "        queue = multiprocessing.Queue()\n",
    "        workers = []\n",
    "\n",
    "        for i in range(self.num_threads - 1):\n",
    "            worker_args = (i + 1, queue, self.env, self.policy, self.custom_reward, mean_action,\n",
    "                           False, self.running_state, thread_batch_size)\n",
    "            workers.append(multiprocessing.Process(target=collect_samples, args=worker_args))\n",
    "        for worker in workers:\n",
    "            worker.start()\n",
    "\n",
    "        memory, log = collect_samples(0, None, self.env, self.policy, self.custom_reward, mean_action,\n",
    "                                      render, self.running_state, thread_batch_size)\n",
    "\n",
    "        worker_logs = [None] * len(workers)\n",
    "        worker_memories = [None] * len(workers)\n",
    "        for _ in workers:\n",
    "            pid, worker_memory, worker_log = queue.get()\n",
    "            worker_memories[pid - 1] = worker_memory\n",
    "            worker_logs[pid - 1] = worker_log\n",
    "        for worker_memory in worker_memories:\n",
    "            memory.append(worker_memory)\n",
    "        batch = memory.sample()\n",
    "        if self.num_threads > 1:\n",
    "            log_list = [log] + worker_logs\n",
    "            log = merge_log(log_list)\n",
    "        to_device(self.device, self.policy)\n",
    "        t_end = time.time()\n",
    "        log['sample_time'] = t_end - t_start\n",
    "        log['action_mean'] = np.mean(np.vstack(batch.action), axis=0)\n",
    "        log['action_min'] = np.min(np.vstack(batch.action), axis=0)\n",
    "        log['action_max'] = np.max(np.vstack(batch.action), axis=0)\n",
    "        return batch, log\n",
    "\n",
    "    def collect_test_trajs(self, target_ind_feat, target_ind_emp, mean_action=False, render=False):\n",
    "        to_device(torch.device('cpu'), self.policy)\n",
    "        thread_batch_size = int(math.ceil(target_ind_feat.shape[0] / self.num_threads))\n",
    "        batch_ind_feat = [target_ind_feat[i*thread_batch_size:min((i+1)*thread_batch_size, target_ind_feat.shape[0])]for i in range(self.num_threads)]\n",
    "        batch_ind_emp = [target_ind_emp[i*thread_batch_size:min((i+1)*thread_batch_size, target_ind_emp.shape[0])]for i in range(self.num_threads)]\n",
    "        queue = multiprocessing.Queue()\n",
    "        workers = []\n",
    "        for i in range(self.num_threads - 1):\n",
    "            worker_args = (i+1, batch_ind_feat[i+1], batch_ind_emp[i+1], queue, self.env, self.policy, self.custom_reward, mean_action,\n",
    "                        False, self.running_state)\n",
    "            workers.append(multiprocessing.Process(target=collect_test_trajs, args=worker_args))\n",
    "        for worker in workers:\n",
    "            worker.start()\n",
    "\n",
    "        trajs = collect_test_trajs(0, batch_ind_feat[0], batch_ind_emp[0], None, self.env, self.policy, self.custom_reward, mean_action,\n",
    "                        render, self.running_state)\n",
    "\n",
    "        worker_trajs = [None] * len(workers)\n",
    "        for _ in workers:\n",
    "            pid, worker_traj = queue.get()\n",
    "            worker_trajs[pid - 1] = worker_traj\n",
    "        for worker_traj in worker_trajs:\n",
    "            trajs = trajs + worker_traj\n",
    "        to_device(self.device, self.policy)\n",
    "        return trajs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TJikc_XodYXp"
   },
   "source": [
    "COMMON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "uX20SJqddCkv"
   },
   "outputs": [],
   "source": [
    "def estimate_advantages(rewards, masks, values, next_values, gamma, tau, device):\n",
    "    rewards, masks, values, next_values = to_device(torch.device('cpu'), rewards, masks, values, next_values)\n",
    "    tensor_type = type(rewards)\n",
    "    deltas = tensor_type(rewards.size(0), 1)\n",
    "    advantages = tensor_type(rewards.size(0), 1)\n",
    "    # prev_value = 0\n",
    "    prev_advantage = 0\n",
    "    for i in reversed(range(rewards.size(0))):\n",
    "        deltas[i] = rewards[i] + gamma * next_values[i] - values[i] # 这一步应该是需要的 因为我等于是到这一步之后 还有一个action 需要这个action是对的才行\n",
    "        advantages[i] = deltas[i] + gamma * tau * prev_advantage * masks[i]\n",
    "        # prev_value = values[i, 0]\n",
    "        prev_advantage = advantages[i, 0]\n",
    "\n",
    "    returns = values + advantages\n",
    "    advantages = (advantages - advantages.mean()) / advantages.std()\n",
    "\n",
    "    advantages, returns = to_device(device, advantages, returns)\n",
    "    return advantages, returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DXZIvRHtdaEI"
   },
   "source": [
    "PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "bsdEINOjdIO_"
   },
   "outputs": [],
   "source": [
    "def ppo_step(policy_net, value_net, optimizer_policy, optimizer_value, optim_value_iternum,\n",
    "             activities, times, durs, traj_lens, dur_leave_homes, dur_travels, next_activities, ind_feats, ind_emps,\n",
    "             returns, advantages, fixed_log_probs, clip_epsilon, l2_reg, max_grad_norm):\n",
    "    value_loss, policy_surr = None, None\n",
    "    \"\"\"update critic\"\"\"\n",
    "    for _ in range(optim_value_iternum):\n",
    "        values_pred = value_net(activities, times, durs, traj_lens, dur_leave_homes, dur_travels, ind_feats, ind_emps)\n",
    "        # print('values_pred', values_pred)\n",
    "        # print('returns', returns)\n",
    "        value_loss = (values_pred - returns).pow(2).mean()\n",
    "        # weight decay\n",
    "        # for param in value_net.parameters():\n",
    "        #     value_loss += param.pow(2).sum() * l2_reg\n",
    "        optimizer_value.zero_grad()\n",
    "        value_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(value_net.parameters(), max_grad_norm)\n",
    "        optimizer_value.step()\n",
    "\n",
    "    \"\"\"update policy\"\"\"\n",
    "    log_probs = policy_net.get_log_prob(activities, times, durs, traj_lens, dur_leave_homes, dur_travels, ind_feats, ind_emps, next_activities)\n",
    "    # print(\"==========================done ppo log_probs\")\n",
    "    ratio = torch.exp(log_probs - fixed_log_probs)\n",
    "    surr1 = ratio * advantages\n",
    "    surr2 = torch.clamp(ratio, 1.0 - clip_epsilon, 1.0 + clip_epsilon) * advantages\n",
    "    policy_surr = -torch.min(surr1, surr2).mean()\n",
    "    optimizer_policy.zero_grad()\n",
    "    policy_surr.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(policy_net.parameters(), max_grad_norm)\n",
    "    optimizer_policy.step()\n",
    "\n",
    "    return value_loss, policy_surr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CSh9aonBeNiz"
   },
   "source": [
    "## GAIL GYM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "aN5Lkc1reP6x"
   },
   "outputs": [],
   "source": [
    "def update_params_airl(batch, i_iter):\n",
    "# Transition = namedtuple('Transition', ('activity', 'time', 'dur', 'action', 'next_activity', 'next_time', 'next_dur', 'reward', 'mask', 'ind_feat', 'ind_emp'))\n",
    "    activities = torch.from_numpy(np.stack(batch.activity)).long().to(device)\n",
    "    times = torch.from_numpy(np.stack(batch.time)).long().to(device)\n",
    "    durs = torch.from_numpy(np.stack(batch.dur)).long().to(device)\n",
    "    traj_lens = torch.from_numpy(np.stack(batch.traj_len)).long().to(device)\n",
    "    dur_leave_homes = torch.from_numpy(np.stack(batch.dur_leave_home)).long().to(device)\n",
    "    dur_travels = torch.from_numpy(np.stack(batch.dur_travel)).long().to(device)\n",
    "    actions = torch.from_numpy(np.stack(batch.action)).long().to(device)\n",
    "    next_activities = torch.from_numpy(np.stack(batch.next_activity)).long().to(device)\n",
    "    next_times = torch.from_numpy(np.stack(batch.next_time)).long().to(device)\n",
    "    next_durs = torch.from_numpy(np.stack(batch.next_dur)).long().to(device)\n",
    "    next_traj_lens = torch.from_numpy(np.stack(batch.next_traj_len)).long().to(device)\n",
    "    next_dur_leave_homes = torch.from_numpy(np.stack(batch.next_dur_leave_home)).long().to(device)\n",
    "    next_dur_travels = torch.from_numpy(np.stack(batch.next_dur_travel)).long().to(device)\n",
    "    masks = torch.from_numpy(np.stack(batch.mask)).long().to(device)\n",
    "    ind_feats = torch.from_numpy(np.stack(batch.ind_feat)).float().to(device)\n",
    "    ind_emps = torch.from_numpy(np.stack(batch.ind_emp)).long().to(device)\n",
    "    # if torch.sum((activities < 7) * (activities != next_activities) * (next_activities != 7)) > 0:\n",
    "    #   print(\"!!!wrong sample!!!\")\n",
    "    with torch.no_grad():\n",
    "        values = value_net(activities, times, durs, traj_lens, dur_leave_homes, dur_travels, ind_feats, ind_emps)\n",
    "        next_values = value_net(next_activities, next_times, next_durs, next_traj_lens, next_dur_leave_homes, next_dur_travels, ind_feats, ind_emps)\n",
    "        # print(\"=====================================start fixed_log_probs\")\n",
    "        fixed_log_probs = policy_net.get_log_prob(activities, times, durs, traj_lens, dur_leave_homes, dur_travels, ind_feats, ind_emps, next_activities)\n",
    "        # print(\"=====================================done fixed_log_probs\")\n",
    "\n",
    "    \"\"\"update discriminator\"\"\"\n",
    "    e_o, g_o, discrim_loss = None, None, None\n",
    "    for _ in range(epoch_disc):\n",
    "        # randomly select a batch from expert_traj\n",
    "        indices = torch.from_numpy(np.random.choice(expert_activity.shape[0], min(activities.shape[0], expert_activity.shape[0]), replace=False)).long()\n",
    "        # print('indices', indices.shape)\n",
    "        s_expert_activity = expert_activity[indices]\n",
    "        s_expert_time = expert_time[indices]\n",
    "        s_expert_dur = expert_dur[indices]\n",
    "        s_expert_traj_len = expert_traj_len[indices]\n",
    "        s_expert_dur_leave_home = expert_dur_leave_home[indices]\n",
    "        s_expert_dur_travel = expert_dur_travel[indices]\n",
    "\n",
    "        s_expert_action = expert_action[indices]\n",
    "\n",
    "        s_expert_next_activity = expert_next_activity[indices]\n",
    "        s_expert_next_time = expert_next_time[indices]\n",
    "        s_expert_next_dur = expert_next_dur[indices]\n",
    "        s_expert_next_traj_len = expert_next_traj_len[indices]\n",
    "        s_expert_next_dur_leave_home = expert_next_dur_leave_home[indices]\n",
    "        s_expert_next_dur_travel = expert_next_dur_travel[indices]\n",
    "\n",
    "        s_expert_ind_feat = expert_ind_feat[indices]\n",
    "        s_expert_ind_emp = expert_ind_emp[indices]\n",
    "        # s_expert_des = expert_des[indices]\n",
    "        # s_expert_ac = expert_ac[indices]\n",
    "        # s_expert_next_st = expert_next_st[indices]\n",
    "        with torch.no_grad():\n",
    "            # print(\"=====================================start expert_log_probs\")\n",
    "            expert_log_probs = policy_net.get_log_prob(s_expert_activity, s_expert_time, s_expert_dur, s_expert_traj_len, s_expert_dur_leave_home, s_expert_dur_travel, \\\n",
    "                                                       s_expert_ind_feat, s_expert_ind_emp, s_expert_next_activity)\n",
    "            # print(\"=====================================done expert_log_probs\")\n",
    "        # states, des, log_pis, next_states\n",
    "        # curr_activity, curr_tim, curr_dur, act, next_activity, next_tim, next_dur, ind_feat, ind_emp, log_pis\n",
    "        g_o = discrim_net(activities, times, durs, traj_lens, dur_leave_homes, dur_travels, actions, \\\n",
    "                          next_activities, next_times, next_durs, next_traj_lens, next_dur_leave_homes, next_dur_travels, \\\n",
    "                          ind_feats, ind_emps, fixed_log_probs)\n",
    "        e_o = discrim_net(s_expert_activity, s_expert_time, s_expert_dur, s_expert_traj_len, s_expert_dur_leave_home, s_expert_dur_travel, s_expert_action, \\\n",
    "                          s_expert_next_activity, s_expert_next_time, s_expert_next_dur, s_expert_next_traj_len, s_expert_next_dur_leave_home, \\\n",
    "                          s_expert_next_dur_travel, s_expert_ind_feat, s_expert_ind_emp, expert_log_probs)\n",
    "        loss_pi = -F.logsigmoid(-g_o).mean()\n",
    "        loss_exp = -F.logsigmoid(e_o).mean()\n",
    "        discrim_loss = loss_pi + loss_exp\n",
    "        optimizer_discrim.zero_grad()\n",
    "        discrim_loss.backward()\n",
    "        optimizer_discrim.step()\n",
    "\n",
    "    expert_acc = ((e_o < 0.5).float()).mean()\n",
    "    learner_acc = ((g_o > 0.5).float()).mean()\n",
    "    # print(i_iter, 'expert_acc', expert_acc.item(), 'learner_acc', learner_acc.item(), 'loss', discrim_loss.item())\n",
    "    # print('done update discriminator...')\n",
    "\n",
    "    \"\"\"get advantage estimation from the trajectories\"\"\"\n",
    "    # states, des, log_pis, next_states\n",
    "    rewards = discrim_net.calculate_reward(activities, times, durs, traj_lens, dur_leave_homes, dur_travels, actions, \\\n",
    "                        next_activities, next_times, next_durs, next_traj_lens, next_dur_leave_homes, next_dur_travels, \\\n",
    "                        ind_feats, ind_emps, fixed_log_probs).squeeze()\n",
    "    # values = torch.zeros_like(rewards).to(rewards.device)\n",
    "    # next_values = torch.zeros_like(rewards).to(rewards.device)\n",
    "    advantages, returns = estimate_advantages(rewards, masks, values, next_values, gamma, tau, device)\n",
    "\n",
    "    \"\"\"perform mini-batch PPO update\"\"\"\n",
    "    value_loss, policy_loss = 0, 0\n",
    "    optim_iter_num = int(math.ceil(activities.shape[0] / optim_batch_size))\n",
    "    for _ in range(optim_epochs):\n",
    "        value_loss, policy_loss = 0, 0\n",
    "        perm = np.arange(activities.shape[0])\n",
    "        np.random.shuffle(perm)\n",
    "        perm = torch.LongTensor(perm).to(device)\n",
    "\n",
    "        activities, times, durs, traj_lens, dur_leave_homes, dur_travels, next_activities, ind_feats, ind_emps, returns, advantages, fixed_log_probs = \\\n",
    "            activities[perm].clone(), times[perm].clone(), durs[perm].clone(), \\\n",
    "            traj_lens[perm].clone(), dur_leave_homes[perm].clone(), dur_travels[perm].clone(), \\\n",
    "            next_activities[perm].clone(), ind_feats[perm].clone(), ind_emps[perm].clone(), \\\n",
    "            returns[perm].clone(), advantages[perm].clone(), fixed_log_probs[perm].clone()\n",
    "\n",
    "        # if torch.sum((activities < 7) * (activities != next_activities) * (next_activities != 7)) > 0:\n",
    "        #   print(\"!!!wrong sample after shuffle!!!\")\n",
    "\n",
    "        for i in range(optim_iter_num):\n",
    "            ind = slice(i * optim_batch_size, min((i + 1) * optim_batch_size, activities.shape[0]))\n",
    "            activities_b, times_b, durs_b, traj_lens_b, dur_leave_homes_b, dur_travels_b, next_activities_b, ind_feats_b, ind_emps_b, returns_b, advantages_b, fixed_log_probs_b = \\\n",
    "                activities[ind], times[ind], durs[ind], traj_lens[ind], dur_leave_homes[ind], dur_travels[ind], next_activities[ind], \\\n",
    "                ind_feats[ind], ind_emps[ind], returns[ind], advantages[ind], fixed_log_probs[ind]\n",
    "\n",
    "            # if torch.sum((activities_b < 7) * (activities_b != next_activities_b) * (next_activities_b != 7)) > 0:\n",
    "            #     print(\"!!!wrong sample after slice!!!\")\n",
    "            batch_value_loss, batch_policy_loss = ppo_step(policy_net, value_net, optimizer_policy, optimizer_value, 1, \\\n",
    "                                      activities_b, times_b, durs_b, traj_lens_b, dur_leave_homes_b, dur_travels_b, \\\n",
    "                                      next_activities_b, ind_feats_b, ind_emps_b, \\\n",
    "                                      returns_b, advantages_b, fixed_log_probs_b, clip_epsilon, l2_reg, \\\n",
    "                                      max_grad_norm)\n",
    "            value_loss += batch_value_loss.item()\n",
    "            policy_loss += batch_policy_loss.item()\n",
    "    # print(i_iter, 'value_loss', value_loss, 'policy_loss', policy_loss)\n",
    "    return discrim_loss.item(), value_loss, policy_loss\n",
    "\n",
    "def save_model(model_path):\n",
    "    policy_statedict = policy_net.state_dict()\n",
    "    value_statedict = value_net.state_dict()\n",
    "    discrim_statedict = discrim_net.state_dict()\n",
    "\n",
    "    outdict = {\"Policy\": policy_statedict,\n",
    "               \"Value\": value_statedict,\n",
    "               \"Discrim\": discrim_statedict}\n",
    "    torch.save(outdict, model_path)\n",
    "\n",
    "\n",
    "def load_model(model_path):\n",
    "    model_dict = torch.load(model_path)\n",
    "    policy_net.load_state_dict(model_dict['Policy'])\n",
    "    print(\"Policy Model loaded Successfully\")\n",
    "    value_net.load_state_dict(model_dict['Value'])\n",
    "    print(\"Value Model loaded Successfully\")\n",
    "    discrim_net.load_state_dict(model_dict['Discrim'])\n",
    "    print(\"Discrim Model loaded Successfully\")\n",
    "\n",
    "def main_loop():\n",
    "    discrim_ls, value_ls, policy_ls, results_ls = [], [], [], []\n",
    "    # learner_trajs = agent.collect_routes_with_OD(test_od, mean_action=True)\n",
    "    # edit_dist, bleu_score, js_dist = evaluate_model(test_trajs, learner_trajs)\n",
    "    best_accuracy = 0.0\n",
    "    for i_iter in range(1, max_iter_num + 1):\n",
    "        # load_model(model_p)\n",
    "        # print(\"======i_iter: \", i_iter)\n",
    "        \"\"\"generate multiple trajectories that reach the minimum batch_size\"\"\"\n",
    "        discrim_net.to(torch.device('cpu'))\n",
    "        t_start = time.time()\n",
    "        batch, log = agent.collect_samples(min_batch_size, mean_action=False)\n",
    "        # print('collect samples', time.time() - t_start)\n",
    "        # print('done collect samples...')\n",
    "        discrim_net.to(device)\n",
    "\n",
    "        discrim_loss, value_loss, policy_loss = update_params_airl(batch, i_iter)\n",
    "        # print(\"======loss: \", {\"discrim_loss\": discrim_loss, \"value_loss\": value_loss, \"policy_loss\": policy_loss})\n",
    "        discrim_ls.append(discrim_loss)\n",
    "        value_ls.append(value_loss)\n",
    "        policy_ls.append(policy_loss)\n",
    "\n",
    "        if i_iter % log_interval == 0:\n",
    "            t_start = time.time()\n",
    "            learner_traj_tuples = agent.collect_test_trajs(test_ind_feat, test_ind_emp, mean_action=False)\n",
    "            # print('generate test route', time.time() - t_start)\n",
    "            # for learner in learner_trajs:\n",
    "            #     print(learner)\n",
    "            # print(learner_traj_tuples[0])\n",
    "            learner_trajs = generated_tuple2seq(learner_traj_tuples)\n",
    "            results = evaluation(test_trajs, learner_trajs, env.time_size)\n",
    "            print(\"======i_iter: \", i_iter)\n",
    "            print(\"======loss: \", {\"discrim_loss\": discrim_loss, \"value_loss\": value_loss, \"policy_loss\": policy_loss})\n",
    "            print(f\"======time: {time.time()-t_start}, results: {results}\")\n",
    "            results_ls.append(results)\n",
    "            if results[\"accuracy\"] > best_accuracy:\n",
    "                best_accuracy = results[\"accuracy\"]\n",
    "                save_model(model_p)\n",
    "            # else:\n",
    "            #   load_model(model_p)\n",
    "            loss_ar = np.array([discrim_ls, value_ls, policy_ls])\n",
    "            np.save(loss_p, loss_ar)\n",
    "            # results_ls\n",
    "            # res_ar = np.array([edit_ls, bleu_ls, js_ls])\n",
    "            # np.save(res_p, res_ar)\n",
    "            with open(res_p, 'wb') as handle:\n",
    "              pickle.dump(results_ls, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "        # if i_iter % 100 == 1:\n",
    "        #     save_model(F\"/content/gdrive/My Drive/route_choice_modeling/model/SH_downtown_selected/airl_model_transfer_%d.pt\"%i_iter)\n",
    "\n",
    "\n",
    "def hard_update(target, source):\n",
    "    \"\"\"\n",
    "    Copies the parameters from source network to target network\n",
    "    :param target: Target network (PyTorch)\n",
    "    :param source: Source network (PyTorch)\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    for target_param, param in zip(target.parameters(), source.parameters()):\n",
    "        target_param.data.copy_(param.data)\n",
    "\n",
    "\n",
    "def plot_loss(loss_path):\n",
    "    matplotlib.rcParams.update({'font.size': 20})\n",
    "    loss = np.load(loss_path)\n",
    "    ylabel_list = ['discrim_loss', 'value_loss', 'policy_loss']\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(24, 8))\n",
    "    for i in range(3):\n",
    "        axs[i].plot(loss[i])\n",
    "        axs[i].set_xlabel('epoch')\n",
    "        axs[i].set_ylabel(ylabel_list[i])\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "    print(1)\n",
    "\n",
    "\n",
    "# def plot_res(res_path):\n",
    "#     matplotlib.rcParams.update({'font.size': 20})\n",
    "#     loss = np.load(res_path)\n",
    "#     print(loss.min(1))\n",
    "#     print(loss.max(1))\n",
    "#     # print(loss)\n",
    "#     ylabel_list = ['edit_dist', 'bleu_score', 'js_distance']\n",
    "#     fig, axs = plt.subplots(1, 3, figsize=(24, 8))\n",
    "#     for i in range(3):\n",
    "#         axs[i].plot(loss[i])\n",
    "#         axs[i].set_xlabel('epoch/10')\n",
    "#         axs[i].set_ylabel(ylabel_list[i])\n",
    "#     fig.tight_layout()\n",
    "#     plt.show()\n",
    "#     print(1)\n",
    "\n",
    "\n",
    "# def filter_single_d(train_path):\n",
    "#     # find the most visited destination in train data\n",
    "#     df = pd.read_csv(train_path)\n",
    "#     print(len(df))\n",
    "#     df = df[['des', 'path']]\n",
    "#     df = df.groupby('des').count()\n",
    "#     df.sort_values(by='path', ascending=False, inplace=True)\n",
    "#     print(df.head())\n",
    "\n",
    "# def ini_od_dist(train_path):\n",
    "#     # find the most visited destination in train data\n",
    "#     df = pd.read_csv(train_path)\n",
    "#     num_trips = len(df)\n",
    "#     df['od'] = df.apply(lambda row: '%d_%d' % (row['ori'], row['des']), axis=1)\n",
    "#     df = df[['od', 'path']]\n",
    "#     df = df.groupby('od').count()\n",
    "#     df['path'] = df['path'] / num_trips\n",
    "#     print(df['path'].sum())\n",
    "#     return df.index.tolist(), df['path'].tolist()\n",
    "\n",
    "\n",
    "# def load_path_feature(path_feature_path):\n",
    "#     # 这样子等于多一个mask 就是走这条路你到不了那个地方\n",
    "#     path_feature = np.load(path_feature_path)\n",
    "#     path_feature_flat = path_feature.reshape(-1, path_feature.shape[2])\n",
    "#     path_feature_max, path_feature_min = np.max(path_feature_flat, 0), np.min(path_feature_flat, 0)\n",
    "#     # path_feature = (path_feature - path_feature_min) / (path_feature_max - path_feature_min + 1e-8)\n",
    "#     # path_feature = 2 * path_feature - 1\n",
    "#     print('path_feature', path_feature.shape)\n",
    "#     return path_feature, path_feature_max, path_feature_min\n",
    "\n",
    "\n",
    "# def load_link_feature(edge_path):\n",
    "#     edge_df = pd.read_csv(edge_path, usecols=['highway', 'length', 'n_id'], dtype={'highway': str})\n",
    "#     # if highway is a list, we define it as the first element of the list\n",
    "#     edge_df['highway'] = edge_df['highway'].apply(lambda loc: (loc.split(',')[0])[2:-1] if ',' in loc else loc)\n",
    "#     level2idx = {'residential': 0, 'primary': 1, 'unclassified': 2, 'tertiary': 3, 'living_street': 4, 'secondary': 5}\n",
    "#     edge_df['highway_idx'] = edge_df['highway'].apply(lambda loc: level2idx.get(loc,2))\n",
    "#     highway_idx = np.eye(6)[edge_df['highway_idx'].values]\n",
    "#     edge_feature = np.concatenate([np.expand_dims(edge_df['length'].values, 1), highway_idx], 1)\n",
    "#     edge_feature_max, edge_feature_min = np.max(edge_feature, 0), np.min(edge_feature, 0)\n",
    "#     # edge_feature = (edge_feature - edge_feature_min) / (edge_feature_max - edge_feature_min + 1e-8)\n",
    "#     # edge_feature = 2 * edge_feature - 1\n",
    "#     print('edge_feature', edge_feature.shape)\n",
    "#     return edge_feature, edge_feature_max, edge_feature_min\n",
    "\n",
    "# def minmax_normalization(feature, xmin, xmax):\n",
    "#     n_feature = (feature - xmin) / (xmax - xmin + 1e-8)\n",
    "#     n_feature = 2 * n_feature - 1\n",
    "#     return n_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fHooCl1keWl6"
   },
   "source": [
    "## Load the pre-trained AIRL model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3JCe-Y1Lkzrc",
    "outputId": "1737beda-e6ab-439c-e259-cf298443e424"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done construct env...\n",
      "done define actor and critic...\n",
      "done load expert data... num of episode: 1477632\n",
      "done construct agent...\n",
      "Policy Model loaded Successfully\n",
      "Value Model loaded Successfully\n",
      "Discrim Model loaded Successfully\n",
      "act_seq (3849, 96)\n",
      "act_seq (3849, 96)\n",
      "======results: {'accuracy': 0.8052849224906902, 'f1-score': 0.5104491820034788, 'edit_dist': 0.18823882826708235, 'bleu_score': 0.8570166758621737, 'data_jsd': 0.6251174352880488, 'macro_int': 0.31989864952690494, 'micro_int': 0.3847793880315357, 'act_type': 0.09216397547958113, 'uni_act_type': 0.1259685922813439, 'traj_len': 0.3255968326371583, 'macro_hour': 0.25874884436447143, 'micro_hour': 0.3499286665180595}\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    irl = 'airl'  # 'airl'\n",
    "\n",
    "    log_std = -0.0  # log std for the policy (default: -0.0)\n",
    "    gamma = 0.99  # discount factor (default: 0.99)\n",
    "    tau = 0.95  # gae (default: 0.95) GAE: generalized\n",
    "    l2_reg = 1e-3  # l2 regularization regression (default: 1e-3)\n",
    "    learning_rate = 3e-4  # gae (default: 3e-4)\n",
    "    clip_epsilon = 0.2  # clipping epsilon for PPO\n",
    "    num_threads = 4  # number of threads for agent (default: 4)\n",
    "    min_batch_size = 8192 # 8192\n",
    "    eval_batch_size = 8192 # 8192\n",
    "    max_iter_num = 3000  # 500  # maximal number of main iterations (default: 500)\n",
    "    log_interval = 10  # interval between training status logs (default: 10)\n",
    "    save_mode_interval = 50  # interval between saving model (default: 0, means don't save)\n",
    "    max_grad_norm = 10\n",
    "    seed = 0\n",
    "    epoch_disc = 1\n",
    "\n",
    "    \"\"\"environment\"\"\"\n",
    "    # \"../data/singapore/train_act_trip_data_filter_user.csv\" \n",
    "    train_p = \"../data/singapore/train_act_trip_data_filter_user.csv\" \n",
    "    # \"../data/singapore/test_act_trip_data_filter_user.csv\" # \n",
    "    test_p = \"../data/singapore/test_act_trip_data_filter_user.csv\"\n",
    "    model_p = \"../model/airl4act_wdwk.pt\" # \"../model/airl_uniq_act.pt\"\n",
    "    loss_p = \"../model/airl4act_loss_wdwk.npy\" # \"../model/airl_uniq_act_loss.npy\"\n",
    "    res_p = \"../result/airl4act_res_wdwk.pkl\" # \"../result/airl_uniq_act_res.npy\"\n",
    "\n",
    "    \"\"\"inialize road environment\"\"\"\n",
    "    env = ActEnvironment(time_size=96, pad_activity_id=5, pad_time_id=96, pad_dur_id=96, pad_traj_len_id=96, pad_dur_leave_home_id=96, pad_dur_travel_id=96)\n",
    "    print('done construct env...')\n",
    "\n",
    "    \"\"\"seeding\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    \"\"\"define actor and critic\"\"\"\n",
    "    policy_net = PolicyNet(activity_size=5, activity_emb_size=5, tim_size=96, tim_emb_size=8, dur_size=96, dur_emb_size=8,\n",
    "                  traj_len_size=96, traj_len_emb_size=8, dur_leave_home_size=96, dur_leave_home_emb_size=8, dur_travel_size=96, dur_travel_emb_size=8,\n",
    "                  emp_num_size=5, emp_size=11, emp_emb_size=5, dropout_p=0.0, gamma=gamma).to(device)\n",
    "    value_net = ValueNet(activity_size=5, activity_emb_size=5, tim_size=96, tim_emb_size=8, dur_size=96, dur_emb_size=8,\n",
    "                  traj_len_size=96, traj_len_emb_size=8, dur_leave_home_size=96, dur_leave_home_emb_size=8, dur_travel_size=96, dur_travel_emb_size=8,\n",
    "                  emp_num_size=5, emp_size=11, emp_emb_size=5, dropout_p=0.0, gamma=gamma).to(device)\n",
    "    discrim_net = DiscriminatorAIRL(activity_size=5, activity_emb_size=5, tim_size=96, tim_emb_size=8, dur_size=96, dur_emb_size=8,\n",
    "                  traj_len_size=96, traj_len_emb_size=8, dur_leave_home_size=96, dur_leave_home_emb_size=8, dur_travel_size=96, dur_travel_emb_size=8,\n",
    "                  act_size=2, act_emb_size=2, emp_num_size=5, emp_size=11, emp_emb_size=8, dropout_p=0.0, gamma=gamma).to(device)\n",
    "\n",
    "    discrim_criterion = nn.BCELoss()\n",
    "    optimizer_policy = torch.optim.Adam(policy_net.parameters(), lr=learning_rate)\n",
    "    optimizer_value = torch.optim.Adam(value_net.parameters(), lr=learning_rate)\n",
    "    optimizer_discrim = torch.optim.Adam(discrim_net.parameters(), lr=learning_rate)\n",
    "    print('done define actor and critic...')\n",
    "    # optimization epoch number and batch size for PPO\n",
    "    optim_epochs = 10\n",
    "    optim_batch_size = 64\n",
    "\n",
    "    \"\"\"load expert trajectory\"\"\"\n",
    "    expert_activity, expert_time, expert_dur, expert_traj_len, expert_dur_leave_home, expert_dur_travel, expert_action, \\\n",
    "          expert_next_activity, expert_next_time, expert_next_dur, expert_next_traj_len, expert_next_dur_leave_home, expert_next_dur_travel, \\\n",
    "          expert_ind_feat, expert_ind_emp = env.import_demonstrations(train_p, test_p, traj_col=\"nonwork_act_trip_seq96\")\n",
    "    expert_activity, expert_time, expert_dur, expert_traj_len, expert_dur_leave_home, expert_dur_travel, expert_action, \\\n",
    "          expert_next_activity, expert_next_time, expert_next_dur, expert_next_traj_len, expert_next_dur_leave_home, expert_next_dur_travel, \\\n",
    "          expert_ind_feat, expert_ind_emp = to_device(device, expert_activity, expert_time, expert_dur, expert_traj_len, expert_dur_leave_home, expert_dur_travel, expert_action, \\\n",
    "          expert_next_activity, expert_next_time, expert_next_dur, expert_next_traj_len, expert_next_dur_leave_home, expert_next_dur_travel, expert_ind_feat, expert_ind_emp)\n",
    "    print('done load expert data... num of episode: %d' % len(expert_activity)) # 842304\n",
    "\n",
    "    \"\"\"load expert trajectory\"\"\"\n",
    "    test_trajs, test_ind_feat, test_ind_emp = env.load_test_traj(train_p, test_p, traj_col=\"nonwork_act_trip_seq96\")\n",
    "\n",
    "    \"\"\"create agent\"\"\"\n",
    "    agent = Agent(env, policy_net, device, custom_reward=None, num_threads=num_threads)\n",
    "    print('done construct agent...')\n",
    "\n",
    "    \"\"\"Train model\"\"\"\n",
    "#     load_model(model_p)\n",
    "#     start_time = time.time()\n",
    "#     main_loop()\n",
    "#     print('train time', time.time()-start_time)\n",
    "\n",
    "    \"\"\"Evaluate model\"\"\"\n",
    "    load_model(model_p)\n",
    "    learner_traj_tuples = agent.collect_test_trajs(test_ind_feat, test_ind_emp, mean_action=False)\n",
    "    learner_trajs = generated_tuple2seq(learner_traj_tuples)\n",
    "    results = evaluation(test_trajs, learner_trajs, env.time_size)\n",
    "    print(f\"======results: {results}\")\n",
    "#     log_prob = eval_log_prob(policy_net, test_trajs, test_ind_feat, test_ind_emp)\n",
    "#     print(f\"======log_prob: {log_prob}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HfFtQD_Nxm97"
   },
   "source": [
    "# Policy Interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E7KRnndj8_Aa"
   },
   "source": [
    "### Logit Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J1mOu1EHxTg-"
   },
   "outputs": [],
   "source": [
    "class PolicyLogitNet(nn.Module):\n",
    "    def __init__(self, activity_size, activity_emb_size,\n",
    "            tim_size, tim_emb_size,\n",
    "            dur_size, dur_emb_size,\n",
    "            traj_len_size, traj_len_emb_size,\n",
    "            dur_leave_home_size, dur_leave_home_emb_size,\n",
    "            dur_travel_size, dur_travel_emb_size,\n",
    "            emp_num_size,\n",
    "            emp_size, emp_emb_size,\n",
    "            dropout_p=0.5, gamma=0.99, cat_feat=True):\n",
    "        super(PolicyLogitNet, self).__init__()\n",
    "\n",
    "        self.device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "        self.gamma = gamma\n",
    "        self.activity_size = activity_size\n",
    "        self.activity_emb_size = activity_emb_size\n",
    "        self.tim_size = tim_size\n",
    "        self.tim_emb_size = tim_emb_size\n",
    "        self.dur_size = dur_size\n",
    "        self.dur_emb_size = dur_emb_size\n",
    "        self.traj_len_size = traj_len_size\n",
    "        self.traj_len_emb_size = traj_len_emb_size\n",
    "        self.dur_leave_home_size = dur_leave_home_size\n",
    "        self.dur_leave_home_emb_size = dur_leave_home_emb_size\n",
    "        self.dur_travel_size = dur_travel_size\n",
    "        self.dur_travel_emb_size = dur_travel_emb_size\n",
    "        self.emp_size = emp_size\n",
    "        self.emp_emb_size = emp_emb_size\n",
    "        self.dropout_p = dropout_p\n",
    "        state_size = self.activity_size + 1 + self.tim_size + 1 + 3\n",
    "        ind_feat_size = self.emp_size + emp_num_size\n",
    "        self.fc = nn.Linear(state_size + ind_feat_size, self.activity_size)\n",
    "\n",
    "    def forward(self, curr_activity, curr_tim, curr_dur, curr_traj_len, curr_dur_leave_home, curr_dur_travel, ind_feat, ind_emp):  # 这是policy\n",
    "        \"\"\"encode features\"\"\"\n",
    "        curr_activity_emb = F.one_hot(curr_activity, num_classes=self.activity_size + 1)\n",
    "        curr_tim_emb = F.one_hot(curr_tim, num_classes=self.tim_size + 1)\n",
    "        curr_dur_emb = (curr_dur / 96.0).float().unsqueeze(-1)\n",
    "        curr_traj_len_emb = (curr_traj_len / 96.0).float().unsqueeze(-1)\n",
    "        curr_dur_leave_home_emb = (curr_dur_leave_home / 96.0).float().unsqueeze(-1)\n",
    "        x_state = torch.cat([curr_activity_emb, curr_tim_emb, curr_dur_emb, curr_traj_len_emb, \n",
    "                             curr_dur_leave_home_emb], -1)\n",
    "        ind_emp_emb = F.one_hot(ind_emp, num_classes=self.emp_size)\n",
    "        ind_feat = torch.cat([ind_feat[:, :-1], ind_emp_emb], -1)\n",
    "        x = torch.cat([x_state, ind_feat], -1)\n",
    "        x = self.fc(x)\n",
    "        prob_x = F.softmax(x, dim=-1)\n",
    "        return prob_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sPzt8de59s8H"
   },
   "source": [
    "### Activity Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0fFDBEY99rhQ"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class ActivityDataset(Dataset):\n",
    "    def __init__(self, train_p, test_p, traj_col, mode, bootstrap, time_size=96, pad_activity_id=5, pad_time_id=96, pad_dur_id=-1,\n",
    "                 pad_traj_len_id=-1, pad_dur_leave_home_id=-1, pad_dur_travel_id=-1):\n",
    "      self.time_size = time_size\n",
    "      self.pad_activity_id = pad_activity_id\n",
    "      self.home_activity_id = 0\n",
    "      self.travel_activity_id = pad_activity_id - 1\n",
    "      self.pad_time_id = pad_time_id\n",
    "      self.pad_dur_id = pad_dur_id\n",
    "      self.pad_traj_len_id = pad_traj_len_id\n",
    "      self.pad_dur_leave_home_id = pad_dur_leave_home_id\n",
    "      self.pad_dur_travel_id = pad_dur_travel_id\n",
    "      self.inputs, self.outputs = self.import_demonstrations(train_p, test_p, traj_col, mode, bootstrap)\n",
    "\n",
    "    def get_reward(self, activity, time, dur):\n",
    "      return 0\n",
    "\n",
    "    def reset(self):\n",
    "      return self.pad_activity_id, self.pad_time_id, self.pad_dur_id, self.pad_traj_len_id, self.pad_dur_leave_home_id, self.pad_dur_travel_id\n",
    "\n",
    "    def step(self, activity, time, dur, traj_len, dur_leave_home, dur_travel, next_activity):\n",
    "      next_time = time + 1 if time < self.pad_time_id else 0\n",
    "      action = 0 if next_activity == activity else 1\n",
    "      next_dur = 0 if action == 1 else dur + 1\n",
    "      next_traj_len = traj_len if action == 0 else (traj_len + 1 if traj_len != self.pad_dur_travel_id else 0)\n",
    "      if next_activity == self.home_activity_id:\n",
    "        next_dur_leave_home = self.pad_dur_leave_home_id\n",
    "      else:\n",
    "        if activity in [self.home_activity_id, self.pad_activity_id]:\n",
    "          next_dur_leave_home = 0\n",
    "        else:\n",
    "          next_dur_leave_home = dur_leave_home + 1\n",
    "      if next_activity == self.travel_activity_id:\n",
    "        if dur_travel == self.pad_dur_travel_id:\n",
    "          next_dur_travel = 0\n",
    "        else:\n",
    "          next_dur_travel = dur_travel + 1\n",
    "      else:\n",
    "        next_dur_travel = dur_travel\n",
    "      reward = self.get_reward(activity, time, dur)\n",
    "      if next_time + 1 == self.time_size:\n",
    "        return next_time, next_dur, next_traj_len, next_dur_leave_home, next_dur_travel, action, reward, True\n",
    "      return next_time, next_dur, next_traj_len, next_dur_leave_home, next_dur_travel, action, reward, False\n",
    "\n",
    "    def import_demonstrations(self, train_p, test_p, traj_col, mode=\"train\", bootstrap=True):\n",
    "      train_df = pd.read_csv(train_p)\n",
    "      test_df = pd.read_csv(test_p)\n",
    "      train_trajs, test_trajs = [], []\n",
    "\n",
    "      train_ind_feat = train_df[[\"age\", \"gender\", \"car\", \"income\", \"workday\"]].values\n",
    "      test_ind_feat = test_df[[\"age\", \"gender\", \"car\", \"income\", \"workday\"]].values\n",
    "      ind_feat = np.concatenate([train_ind_feat, test_ind_feat], 0)\n",
    "      min_ind_feat, max_ind_feat = np.expand_dims(np.min(ind_feat, 0), 0), np.expand_dims(np.max(ind_feat, 0), 0)\n",
    "      if mode == \"train\":\n",
    "        train_ind_feat = (train_ind_feat - min_ind_feat) / (max_ind_feat - min_ind_feat)\n",
    "        train_ind_emp = train_df[\"employ\"].values\n",
    "        train_demo_str_ls = train_df[traj_col].tolist()\n",
    "      else:\n",
    "        train_ind_feat = (test_ind_feat - min_ind_feat) / (max_ind_feat - min_ind_feat)\n",
    "        train_ind_emp = test_df[\"employ\"].values\n",
    "        train_demo_str_ls = test_df[traj_col].tolist()\n",
    "\n",
    "      expert_activity, expert_time, expert_dur, expert_traj_len, expert_dur_leave_home, expert_dur_travel = [], [], [], [], [], []\n",
    "      expert_next_activity, expert_next_time, expert_next_dur, expert_next_traj_len, expert_next_dur_leave_home, expert_next_dur_travel = [], [], [], [], [], []\n",
    "      expert_action, expert_ind_feat, expert_ind_emp = [], [], []\n",
    "\n",
    "      if mode == \"train\" and bootstrap is True:\n",
    "        indices = np.random.randint(0, len(train_demo_str_ls), size=(len(train_demo_str_ls),)).tolist()\n",
    "      else:\n",
    "        indices = range(len(train_demo_str_ls))\n",
    "        \n",
    "      for user in indices:\n",
    "        train_demo_ls = [int(demo) for demo in train_demo_str_ls[user].split(\"_\")]\n",
    "        activity, time, dur, traj_len, dur_leave_home, dur_travel = self.reset()\n",
    "        for i0 in range(len(train_demo_ls)):\n",
    "          next_activity = train_demo_ls[i0]\n",
    "          next_time, next_dur, next_traj_len, next_dur_leave_home, next_dur_travel, action, _, _ = self.step(activity, time, dur, traj_len, dur_leave_home, dur_travel, next_activity)\n",
    "          expert_activity.append(activity)\n",
    "          expert_time.append(time)\n",
    "          expert_dur.append(dur)\n",
    "          expert_traj_len.append(traj_len)\n",
    "          expert_dur_leave_home.append(dur_leave_home)\n",
    "          expert_dur_travel.append(dur_travel)\n",
    "          expert_action.append(action)\n",
    "          expert_next_activity.append(next_activity)\n",
    "          expert_next_time.append(next_time)\n",
    "          expert_next_dur.append(next_dur)\n",
    "          expert_next_traj_len.append(next_traj_len)\n",
    "          expert_next_dur_leave_home.append(next_dur_leave_home)\n",
    "          expert_next_dur_travel.append(next_dur_travel)\n",
    "          expert_ind_feat.append(train_ind_feat[user])\n",
    "          expert_ind_emp.append(train_ind_emp[user])\n",
    "          activity, time, dur, traj_len, dur_leave_home, dur_travel = next_activity, next_time, next_dur, next_traj_len, next_dur_leave_home, next_dur_travel\n",
    "\n",
    "      inputs = dict()\n",
    "      inputs[\"expert_activity\"] = torch.LongTensor(expert_activity).to(device)\n",
    "      inputs[\"expert_time\"] = torch.LongTensor(expert_time).to(device)\n",
    "      inputs[\"expert_dur\"] = torch.LongTensor(expert_dur).to(device)\n",
    "      inputs[\"expert_traj_len\"] = torch.LongTensor(expert_traj_len).to(device)\n",
    "      inputs[\"expert_dur_leave_home\"] = torch.LongTensor(expert_dur_leave_home).to(device)\n",
    "      inputs[\"expert_dur_travel\"] = torch.LongTensor(expert_dur_travel).to(device)\n",
    "      inputs[\"expert_ind_feat\"] = torch.FloatTensor(np.array(expert_ind_feat)).to(device)\n",
    "      inputs[\"expert_ind_emp\"] = torch.LongTensor(np.array(expert_ind_emp)).to(device)\n",
    "      outputs = dict()\n",
    "      outputs[\"expert_action\"] = torch.LongTensor(expert_action).to(device)\n",
    "      outputs[\"expert_next_activity\"] = torch.LongTensor(expert_next_activity).to(device)\n",
    "      outputs[\"expert_next_time\"] = torch.LongTensor(expert_next_time).to(device)\n",
    "      outputs[\"expert_next_dur\"] = torch.LongTensor(expert_next_dur).to(device)\n",
    "      outputs[\"expert_next_traj_len\"] = torch.LongTensor(expert_next_traj_len).to(device)\n",
    "      outputs[\"expert_next_dur_leave_home\"] = torch.LongTensor(expert_next_dur_leave_home).to(device)\n",
    "      outputs[\"expert_next_dur_travel\"] = torch.LongTensor(expert_next_dur_travel).to(device)\n",
    "      return inputs, outputs\n",
    "\n",
    "    def load_test_traj(self, train_p, test_p, traj_col):\n",
    "      train_df = pd.read_csv(train_p)\n",
    "      # train_df = train_df.loc[train_df[filter_col] > 0].copy()\n",
    "      test_df = pd.read_csv(test_p)\n",
    "      # test_df = train_df.loc[test_df[filter_col] > 0].copy()\n",
    "      train_trajs, test_trajs = [], []\n",
    "\n",
    "      train_ind_feat = train_df[[\"age\", \"gender\", \"car\", \"income\", \"workday\"]].values\n",
    "      test_ind_feat = test_df[[\"age\", \"gender\", \"car\", \"income\", \"workday\"]].values\n",
    "      ind_feat = np.concatenate([train_ind_feat, test_ind_feat], 0)\n",
    "      min_ind_feat, max_ind_feat = np.expand_dims(np.min(ind_feat, 0), 0), np.expand_dims(np.max(ind_feat, 0), 0)\n",
    "\n",
    "      # test_df = test_df.sample(n=min(1000, len(test_df)))\n",
    "      test_ind_feat = test_df[[\"age\", \"gender\", \"car\", \"income\", \"workday\"]].values\n",
    "      test_ind_feat = (test_ind_feat - min_ind_feat) / (max_ind_feat - min_ind_feat)\n",
    "      test_ind_emp = test_df[\"employ\"].values\n",
    "      test_demo_str_ls = test_df[traj_col].tolist()\n",
    "      test_traj = [[int(i) for i in path.split('_')] for path in test_demo_str_ls]\n",
    "      return np.array(test_traj), test_ind_feat, test_ind_emp\n",
    "\n",
    "    def __len__(self):\n",
    "      return len(self.inputs[\"expert_activity\"])\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "      return self.inputs[\"expert_activity\"][idx], self.inputs[\"expert_time\"][idx], self.inputs[\"expert_dur\"][idx],\\\n",
    "          self.inputs[\"expert_traj_len\"][idx], self.inputs[\"expert_dur_leave_home\"][idx], self.inputs[\"expert_dur_travel\"][idx],\\\n",
    "          self.inputs[\"expert_ind_feat\"][idx], self.inputs[\"expert_ind_emp\"][idx], self.outputs[\"expert_action\"][idx],\\\n",
    "          self.outputs[\"expert_next_activity\"][idx], self.outputs[\"expert_next_time\"][idx], self.outputs[\"expert_next_dur\"][idx],\\\n",
    "          self.outputs[\"expert_next_traj_len\"][idx], self.outputs[\"expert_next_dur_leave_home\"][idx], self.outputs[\"expert_next_dur_travel\"][idx]\n",
    "\n",
    "class DataGenerator(object):\n",
    "    def __init__(self, train_p, test_p, traj_col, bootstrap=True):\n",
    "      self.train_data = ActivityDataset(train_p, test_p, traj_col, \"train\", bootstrap)\n",
    "      self.test_data = ActivityDataset(train_p, test_p, traj_col, \"test\", False)\n",
    "\n",
    "    def get_data_loader(self, batch_size: int):\n",
    "        data_loader = dict()\n",
    "        data_loader['train'] = DataLoader(dataset=self.train_data, batch_size=batch_size, shuffle=True)\n",
    "        data_loader['test'] = DataLoader(dataset=self.test_data, batch_size=batch_size, shuffle=False)\n",
    "        return data_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pQKSNqYrHhfu"
   },
   "source": [
    "### ModelTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "sQrIeaNaHj8q"
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import torch.optim as optim\n",
    "\n",
    "class ModelTrainer(object):\n",
    "    # we need a pre-trained DIRL model here, or maybe policy_network here\n",
    "    def __init__(self, model:nn.Module, pretrained_model:nn.Module, optimizer, lr:float, wd:float, n_epochs:int):\n",
    "        self.model = model\n",
    "        self.pretrained_model = pretrained_model\n",
    "        self.criterion = nn.NLLLoss(reduction=\"mean\")\n",
    "        self.optimizer = optimizer(params=self.model.parameters(), lr=lr, weight_decay=wd)\n",
    "        self.n_epochs = n_epochs\n",
    "\n",
    "    def train(self, data_processor:dict, modes:list, model_dir:str, early_stopper=10, alpha=1):\n",
    "        checkpoint = {'epoch':0, 'state_dict':self.model.state_dict()}\n",
    "        val_loss = np.inf\n",
    "        start_time = datetime.datetime.now()\n",
    "        self.pretrained_model.eval()\n",
    "        for epoch in range(1, self.n_epochs+1):\n",
    "            running_loss = {mode:0.0 for mode in modes}\n",
    "            for mode in modes:\n",
    "                if mode == 'train':\n",
    "                    self.model.train()\n",
    "                else:\n",
    "                    self.model.eval()\n",
    "                step = 0\n",
    "                for expert_activity, expert_time, expert_dur, expert_traj_len, expert_dur_leave_home, expert_dur_travel, \\\n",
    "                   expert_ind_feat, expert_ind_emp, expert_action, expert_next_activity, expert_next_time, expert_next_dur, \\\n",
    "                   expert_next_traj_len, expert_next_dur_leave_home, expert_next_dur_travel in data_processor[mode]:\n",
    "                    with torch.set_grad_enabled(mode = mode=='train'):\n",
    "                        # pretrained_expert_dur = expert_dur\n",
    "                        # pretrained_expert_dur[pretrained_expert_dur < 0] = 96\n",
    "                        y_pred = self.model(expert_activity, expert_time, expert_dur, expert_traj_len, expert_dur_leave_home, \\\n",
    "                                            expert_dur_travel, expert_ind_feat, expert_ind_emp)\n",
    "                        pretrained_expert_dur = expert_dur\n",
    "                        pretrained_expert_dur[pretrained_expert_dur < 0] = 96\n",
    "                        pretrained_expert_traj_len = expert_traj_len\n",
    "                        pretrained_expert_traj_len[pretrained_expert_traj_len < 0] = 96\n",
    "                        pretrained_expert_dur_leave_home = expert_dur_leave_home\n",
    "                        pretrained_expert_dur_leave_home[pretrained_expert_dur_leave_home < 0] = 96\n",
    "                        pretrained_expert_dur_travel = expert_dur_travel\n",
    "                        pretrained_expert_dur_travel[pretrained_expert_dur_travel < 0] = 96\n",
    "                        y_pred_pretrained = self.pretrained_model.get_action_prob(expert_activity, expert_time, pretrained_expert_dur, \n",
    "                                                                                  pretrained_expert_traj_len, \\\n",
    "                                                      pretrained_expert_dur_leave_home, pretrained_expert_dur_travel, expert_ind_feat, \\\n",
    "                                                      expert_ind_emp)\n",
    "                        \"\"\"CE\"\"\"\n",
    "                        hard_loss = self.criterion(torch.log(y_pred), expert_next_activity)\n",
    "                        soft_loss = -(y_pred_pretrained * torch.log(y_pred + 1e-32)).mean()\n",
    "                        # print(\"hard_loss\", hard_loss.item(), \"soft_loss\", soft_loss.item())\n",
    "                        loss = (1-alpha) * hard_loss + alpha * soft_loss\n",
    "                        if mode == 'train':\n",
    "                            self.optimizer.zero_grad()\n",
    "                            loss.backward()\n",
    "                            self.optimizer.step()\n",
    "\n",
    "                    running_loss[mode] += hard_loss\n",
    "                    step += expert_next_activity.shape[0]\n",
    "                if mode == 'test':\n",
    "                    if running_loss[mode]/step <= val_loss:\n",
    "                        print(f\"test_loss drops from {val_loss} to {running_loss[mode]/step}...\")\n",
    "                        val_loss = running_loss[mode]/step\n",
    "                        checkpoint.update(epoch=epoch, state_dict=self.model.state_dict())\n",
    "                        torch.save(checkpoint, model_dir)\n",
    "                        early_stopper = 10\n",
    "                    else:\n",
    "                        print(f\"test_loss does not drop from {val_loss}...\")\n",
    "                        early_stopper -= 1\n",
    "                        if early_stopper == 0:\n",
    "                          print(f'Early stopping at epoch {epoch}..')\n",
    "                          return\n",
    "        print('training', datetime.datetime.now() - start_time)\n",
    "        return\n",
    "\n",
    "    def test_pretrained(self, activity_dataset, train_p, test_p, traj_col, model_dir:str, max_prob=False):\n",
    "        \"\"\"load model\"\"\"\n",
    "        # saved_checkpoint = torch.load(model_dir)\n",
    "        # self.model.load_state_dict(saved_checkpoint['state_dict'])\n",
    "        # self.model.eval()\n",
    "        self.pretrained_model.eval()\n",
    "        \"\"\"load ground truth data\"\"\"\n",
    "        test_trajs, test_ind_feat, test_ind_emp = activity_dataset.load_test_traj(train_p, test_p, traj_col)\n",
    "        start_time = datetime.datetime.now()\n",
    "        pred_trajs = []\n",
    "        for i in range(test_ind_feat.shape[0]):\n",
    "          activity, time, dur, traj_len, dur_leave_home, dur_travel = activity_dataset.reset()\n",
    "          # , test_ind_emp[i]\n",
    "          ind_feat_var = torch.tensor(test_ind_feat[i]).float().to(device).unsqueeze(0)\n",
    "          ind_emp_var = torch.tensor(test_ind_emp[i]).long().to(device).unsqueeze(0)\n",
    "          pred_traj = []\n",
    "          for t in range(96):  # 这个感觉是maximum_length的意思\n",
    "            activity_var = torch.tensor(activity).long().to(device).unsqueeze(0)\n",
    "            time_var = torch.tensor(time).long().to(device).unsqueeze(0)\n",
    "            dur_var = torch.tensor(dur).long().to(device).unsqueeze(0)\n",
    "            traj_len_var = torch.tensor(traj_len).long().to(device).unsqueeze(0)\n",
    "            dur_leave_home_var = torch.tensor(dur_leave_home).long().to(device).unsqueeze(0)\n",
    "            dur_travel_var = torch.tensor(dur_travel).long().to(device).unsqueeze(0)\n",
    "            \n",
    "            dur_var[dur_var < 0] = 96\n",
    "            traj_len_var[traj_len_var < 0] = 96\n",
    "            dur_leave_home_var[dur_leave_home_var < 0] = 96\n",
    "            dur_travel_var[dur_travel_var < 0] = 96\n",
    "                        \n",
    "            if max_prob:\n",
    "              pred_next_activity = torch.argmax(self.pretrained_model.get_action_prob(activity_var, time_var, dur_var, traj_len_var, dur_leave_home_var, \\\n",
    "                                          dur_travel_var, ind_feat_var, ind_emp_var),-1)\n",
    "            else:\n",
    "              pred_next_activity = self.pretrained_model.select_action(activity_var, time_var, dur_var, traj_len_var, \\\n",
    "                                          dur_leave_home_var, dur_travel_var, ind_feat_var, ind_emp_var)[0]# torch.distributions.Categorical(y_pred).sample()\n",
    "\n",
    "            pred_next_activity = pred_next_activity.item()\n",
    "\n",
    "            next_time, next_dur, next_traj_len, next_dur_leave_home, next_dur_travel, action, _, done = \\\n",
    "                activity_dataset.step(activity, time, dur, traj_len, dur_leave_home, dur_travel, pred_next_activity)\n",
    "            pred_traj.append(pred_next_activity)\n",
    "            activity, time, dur, traj_len, dur_leave_home, dur_travel = pred_next_activity, next_time, next_dur, next_traj_len, next_dur_leave_home, next_dur_travel\n",
    "          pred_trajs.append(pred_traj)\n",
    "\n",
    "        pred_trajs = np.array(pred_trajs)\n",
    "        print(\"test_trajs\", test_trajs.shape)\n",
    "        print(\"pred_trajs\", pred_trajs.shape)\n",
    "        results = evaluation(test_trajs, pred_trajs, 96)\n",
    "        print('test', datetime.datetime.now() - start_time)\n",
    "        print(f\"======results: {results}\")\n",
    "        return test_trajs, pred_trajs\n",
    "    \n",
    "    def test(self, activity_dataset, train_p, test_p, traj_col, model_dir:str, max_prob=False):\n",
    "        \"\"\"load model\"\"\"\n",
    "        saved_checkpoint = torch.load(model_dir)\n",
    "        self.model.load_state_dict(saved_checkpoint['state_dict'])\n",
    "        self.model.eval()\n",
    "        self.pretrained_model.eval()\n",
    "        \"\"\"load ground truth data\"\"\"\n",
    "        test_trajs, test_ind_feat, test_ind_emp = activity_dataset.load_test_traj(train_p, test_p, traj_col)\n",
    "        start_time = datetime.datetime.now()\n",
    "        pred_trajs = []\n",
    "        for i in range(test_ind_feat.shape[0]):\n",
    "          activity, time, dur, traj_len, dur_leave_home, dur_travel = activity_dataset.reset()\n",
    "          # , test_ind_emp[i]\n",
    "          ind_feat_var = torch.tensor(test_ind_feat[i]).float().to(device).unsqueeze(0)\n",
    "          ind_emp_var = torch.tensor(test_ind_emp[i]).long().to(device).unsqueeze(0)\n",
    "          pred_traj = []\n",
    "          for t in range(96):  # 这个感觉是maximum_length的意思\n",
    "            activity_var = torch.tensor(activity).long().to(device).unsqueeze(0)\n",
    "            time_var = torch.tensor(time).long().to(device).unsqueeze(0)\n",
    "            dur_var = torch.tensor(dur).long().to(device).unsqueeze(0)\n",
    "            traj_len_var = torch.tensor(traj_len).long().to(device).unsqueeze(0)\n",
    "            dur_leave_home_var = torch.tensor(dur_leave_home).long().to(device).unsqueeze(0)\n",
    "            dur_travel_var = torch.tensor(dur_travel).long().to(device).unsqueeze(0)\n",
    "            y_pred = self.model(activity_var, time_var, dur_var, traj_len_var, dur_leave_home_var, \\\n",
    "                              dur_travel_var, ind_feat_var, ind_emp_var)\n",
    "            if max_prob:\n",
    "              pred_next_activity = torch.argmax(y_pred,-1)\n",
    "            else:\n",
    "              pred_next_activity = torch.distributions.Categorical(y_pred).sample()\n",
    "\n",
    "            pred_next_activity = pred_next_activity.item()\n",
    "\n",
    "            next_time, next_dur, next_traj_len, next_dur_leave_home, next_dur_travel, action, _, done = \\\n",
    "                activity_dataset.step(activity, time, dur, traj_len, dur_leave_home, dur_travel, pred_next_activity)\n",
    "            pred_traj.append(pred_next_activity)\n",
    "            activity, time, dur, traj_len, dur_leave_home, dur_travel = pred_next_activity, next_time, next_dur, next_traj_len, next_dur_leave_home, next_dur_travel\n",
    "          pred_trajs.append(pred_traj)\n",
    "\n",
    "        pred_trajs = np.array(pred_trajs)\n",
    "        print(\"test_trajs\", test_trajs.shape)\n",
    "        print(\"pred_trajs\", pred_trajs.shape)\n",
    "        results = evaluation(test_trajs, pred_trajs, 96)\n",
    "        print('test', datetime.datetime.now() - start_time)\n",
    "        print(f\"======results: {results}\")\n",
    "        return test_trajs, pred_trajs\n",
    "    \n",
    "    def eval_log_prob(self, activity_dataset, train_p, test_p, traj_col, model_dir:str):\n",
    "        \"\"\"load model\"\"\"\n",
    "        saved_checkpoint = torch.load(model_dir)\n",
    "        self.model.load_state_dict(saved_checkpoint['state_dict'])\n",
    "        self.model.eval()\n",
    "        self.pretrained_model.eval()\n",
    "        \"\"\"load ground truth data\"\"\"\n",
    "        test_trajs, test_ind_feat, test_ind_emp = activity_dataset.load_test_traj(train_p, test_p, traj_col)\n",
    "        start_time = datetime.datetime.now()\n",
    "        log_prob_ls = []\n",
    "        for i in range(test_ind_feat.shape[0]):\n",
    "          activity, time, dur, traj_len, dur_leave_home, dur_travel = activity_dataset.reset()\n",
    "          # , test_ind_emp[i]\n",
    "          ind_feat_var = torch.tensor(test_ind_feat[i]).float().to(device).unsqueeze(0)\n",
    "          ind_emp_var = torch.tensor(test_ind_emp[i]).long().to(device).unsqueeze(0)\n",
    "          log_prob = 0\n",
    "          for t in range(96):  # 这个感觉是maximum_length的意思\n",
    "            activity_var = torch.tensor(activity).long().to(device).unsqueeze(0)\n",
    "            time_var = torch.tensor(time).long().to(device).unsqueeze(0)\n",
    "            dur_var = torch.tensor(dur).long().to(device).unsqueeze(0)\n",
    "            traj_len_var = torch.tensor(traj_len).long().to(device).unsqueeze(0)\n",
    "            dur_leave_home_var = torch.tensor(dur_leave_home).long().to(device).unsqueeze(0)\n",
    "            dur_travel_var = torch.tensor(dur_travel).long().to(device).unsqueeze(0)\n",
    "            y_pred = self.model(activity_var, time_var, dur_var, traj_len_var, dur_leave_home_var, \\\n",
    "                              dur_travel_var, ind_feat_var, ind_emp_var)\n",
    "            pred_next_activity = test_trajs[i,t]\n",
    "            log_prob += np.log(y_pred.detach().cpu().numpy()[0, pred_next_activity])\n",
    "\n",
    "            next_time, next_dur, next_traj_len, next_dur_leave_home, next_dur_travel, action, _, done = \\\n",
    "                activity_dataset.step(activity, time, dur, traj_len, dur_leave_home, dur_travel, pred_next_activity)\n",
    "            activity, time, dur, traj_len, dur_leave_home, dur_travel = pred_next_activity, next_time, next_dur, next_traj_len, next_dur_leave_home, next_dur_travel\n",
    "          log_prob_ls.append(log_prob)\n",
    "\n",
    "        return np.mean(log_prob_ls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d4RN-SIPVr6p"
   },
   "source": [
    "### Train and Predict with Logit Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "mA3QTamhcmSf"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "def setup_seed(seed):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_p = \"../data/singapore/test_act_trip_data_filter_user.csv\"\n",
    "test_p = \"../data/singapore/test_act_trip_data_filter_user.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y-NLl0pXcYez"
   },
   "outputs": [],
   "source": [
    "data_generator = DataGenerator(train_p, test_p, traj_col=\"nonwork_act_trip_seq96\", bootstrap=False)\n",
    "data_processor = data_generator.get_data_loader(batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CWoJaYXKVwqo",
    "outputId": "5b97046d-78fd-472b-8723-06bc87695244"
   },
   "outputs": [],
   "source": [
    "learn_rate = 2e-3\n",
    "weight_decay = 0\n",
    "optimizer = optim.Adam\n",
    "epoch = 50\n",
    "\n",
    "\"\"\"load pre-trained DIRL\"\"\"\n",
    "optimizer = optim.Adam\n",
    "\n",
    "for alpha in [0.5, 0.7, 0.9]:\n",
    "  print(f\"================start:{alpha}\")\n",
    "  for ep in range(1):\n",
    "    setup_seed(0)\n",
    "    logit_model = PolicyLogitNet(activity_size=5, activity_emb_size=5, tim_size=96, tim_emb_size=8, dur_size=96, dur_emb_size=8,\n",
    "                traj_len_size=96, traj_len_emb_size=8, dur_leave_home_size=96, dur_leave_home_emb_size=8, dur_travel_size=96, dur_travel_emb_size=8,\n",
    "                emp_num_size=4, emp_size=11, emp_emb_size=8, dropout_p=0.0, cat_feat=False).to(device)\n",
    "    trainer = ModelTrainer(model=logit_model, pretrained_model=policy_net, optimizer=optimizer, lr=learn_rate, \\\n",
    "                            wd=weight_decay, n_epochs=epoch)\n",
    "    model_dir = f\"../model/logit_model_wdwk_alpha{alpha}.pt\"\n",
    "    st_time = time.time()\n",
    "    trainer.train(data_processor=data_processor, modes=[\"train\", \"test\"], model_dir=model_dir, early_stopper=10, alpha=alpha)\n",
    "    ground_truth, prediction = trainer.test(activity_dataset=data_generator.train_data, train_p=train_p, test_p=test_p, traj_col=\"nonwork_act_trip_seq96\", model_dir=model_dir, max_prob=False)\n",
    "\n",
    "    \"\"\"load and save model parameter\"\"\"\n",
    "    saved_checkpoint = torch.load(model_dir)\n",
    "    logit_model.load_state_dict(saved_checkpoint['state_dict'])\n",
    "    logit_model.eval()\n",
    "    coef = logit_model.fc.weight.detach().cpu().numpy()\n",
    "    bias = logit_model.fc.bias.detach().cpu().numpy()\n",
    "    coef = np.concatenate([coef, bias[:,None]], -1)\n",
    "    \n",
    "    coef = coef[None, :, :]\n",
    "    \n",
    "    n_activity = 5\n",
    "    n_timestep = 96\n",
    "    n_travel_num_feat = 4\n",
    "    n_dem_num_feat = 4\n",
    "    n_employ_type = 11\n",
    "\n",
    "    next_activity_type = [\"Home\", \"Work\", \"School\", \"Others\", \"Travel\"]\n",
    "    coef_col_names = []\n",
    "    coef_activity_type = coef[:, :, :n_activity]\n",
    "    name_activity_type = [\"Curr Act:Home\", \"Curr Act:Work\", \"Curr Act:School\", \n",
    "                          \"Curr Act:Others\", \"Curr Act:Travel\"]\n",
    "\n",
    "    coef_time_type = coef[:, :, n_activity+1:n_activity+n_timestep+1]\n",
    "    coef_time_type = coef_time_type.reshape(coef_time_type.shape[0], coef_time_type.shape[1], 12, -1)\n",
    "    coef_time_type = np.mean(coef_time_type, -1)\n",
    "    name_time_type = [f\"Curr Time:{i}-{i+2}h\" for i in range(0, 24, 2)]\n",
    "\n",
    "    coef_cum_feat = coef[:, :, n_activity+n_timestep+2:n_activity+n_timestep+n_travel_num_feat+2]\n",
    "    name_cum_feat = [\"Stay Duration\", \"Curr Cumulative Act Count\", \"Curr Cumulative Dur Leaving Home\", \"Curr Cumulative Travel Dur\"]\n",
    "\n",
    "    coef_dem_feat = coef[:, :, n_activity+n_timestep+n_travel_num_feat+2:\n",
    "                         n_activity+n_timestep+n_travel_num_feat+n_dem_num_feat+2]\n",
    "    name_dem_feat = [\"Age\", \"Gender\", \"Has Car\", \"Income\"]\n",
    "\n",
    "    coef_emp = coef[:, :, n_activity+n_timestep+n_travel_num_feat+n_dem_num_feat+2:\n",
    "                    n_activity+n_timestep+n_travel_num_feat+n_dem_num_feat+2+n_employ_type]\n",
    "    name_emp = [\"Homemaker\", \"Full-time Student\", \"Voluntary worker\", \"Employed Part-time\", \"Employed Full-time\", \"Non-schooling Child\", \"Self-employed\", \\\n",
    "                       \"National Service\", \"Unemployed\", \"Retired\", \"Domestic Worker\"]\n",
    "\n",
    "    bias = coef[:, :, -1:]\n",
    "    name_bias = [\"Intercept\"]\n",
    "    coef_processed = np.concatenate([coef_activity_type, coef_time_type, coef_cum_feat, \n",
    "                                     coef_dem_feat, coef_emp, bias], -1)\n",
    "    coef_col_names = name_activity_type + name_time_type + name_cum_feat + \\\n",
    "                        name_dem_feat + name_emp + name_bias\n",
    "\n",
    "    data = {\"Variables\": coef_col_names}\n",
    "    for i in range(5):\n",
    "        data[f\"{next_activity_type[i]}_mean\"] = np.mean(coef_processed[:, i], 0)\n",
    "        # data[f\"{next_activity_type[i]}_std\"] = np.std(coef_processed[:, i], 0)\n",
    "        # data[f\"{next_activity_type[i]}_min\"] = np.min(coef_processed[:, i], 0)\n",
    "        # data[f\"{next_activity_type[i]}_max\"] = np.max(coef_processed[:, i], 0)\n",
    "        # data[f\"{next_activity_type[i]}_0025\"] = np.percentile(coef_processed[:, i], 2.5, axis=0)\n",
    "        # data[f\"{next_activity_type[i]}_0975\"] = np.percentile(coef_processed[:, i], 97.5, axis=0)\n",
    "    df = pd.DataFrame(data=data)\n",
    "    df.to_csv(f\"../result/bootstrapp_logit_coef_example_alpha{alpha}.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
